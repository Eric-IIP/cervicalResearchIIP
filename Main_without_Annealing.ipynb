{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "## parameters for experiment\n",
    "N_BLOCK = 5\n",
    "LR = 0.003\n",
    "\n",
    "OUTPUT_DIR = '2025-06-23-MRCNN'\n",
    "\n",
    "IN_CHANNEL = 108\n",
    "\n",
    "AUGMENTED = True\n",
    "AUGMENTATION  =  30\n",
    "\n",
    "\n",
    "CROSS_VAL = True\n",
    "N_SPLIT = 4\n",
    "\n",
    "\n",
    "#optimization constants\n",
    "DICE_IMPORTANCE = 1.0\n",
    "CE_IMPORTANCE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:47:38.069916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-23 15:47:38.758966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "## import libraries\n",
    "from numpy.core.numeric import NaN\n",
    "from MCtool.RFilter import gray\n",
    "from genericpath import exists\n",
    "from matplotlib import image\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras.backend import dtype\n",
    "from DeepLearning import LearnAndTest\n",
    "from Rpkg.Rfund.InputFeature import InputFeature\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Rpkg.Rfund import ReadFile, WriteFile\n",
    "from Rpkg.Rmodel import Unet, Mnet\n",
    "\n",
    "import Filtering\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import DeepLearning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from Rpkg.Rfund.InputFeature import InputFeature\n",
    "from Rpkg.Rfund import ReadFile, WriteFile\n",
    "from Rpkg.Rmodel import Unet, Mnet\n",
    "\n",
    "from MCtool import RFilter, resultEval\n",
    "from DeepLearning import save_eval_result\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from transformations import ComposeDouble, FunctionWrapperDouble, create_dense_target, normalize_01\n",
    "from customdatasets import SegmentationDataSet1\n",
    "from customdatasets import SegmentationDataSet4\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "from skimage.transform import resize\n",
    "\n",
    "#early stopping なし\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random seed config\n",
    "# Make sure there is no randomness in the output so that the output is reproduceable\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for Python random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using GPU\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Make the convolution operations deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Disable the CUDNN benchmark to ensure deterministic results\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version installed: 2.3.0+cu121\n",
      "CUDA version associated with PyTorch version: 12.1\n",
      "Version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch8902\n",
      "CUDA is available: True\n",
      "Number of GPUs compatible with CUDA:1\n",
      "Name of the GPU at index 0: NVIDIA GeForce RTX 2080 Ti\n",
      "Current CUDA device index: 0\n"
     ]
    }
   ],
   "source": [
    "## cuda and pytorch stats\n",
    "# 自分の環境設定がうまくいったかどうかを確認しましょう、特にGPUの動作\n",
    "# Prints the version of PyTorch installed\n",
    "print('PyTorch Version installed: ' + torch.__version__)\n",
    "\n",
    "# Prints the version of CUDA associated with the installed PyTorch version\n",
    "print('CUDA version associated with PyTorch version: ' + torch.version.cuda)\n",
    "\n",
    "# Prints the version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch\n",
    "print('Version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch' + str(torch.backends.cudnn.version()))\n",
    "\n",
    "# Same as the line above\n",
    "print('CUDA is available: ' + str(torch.cuda.is_available()))\n",
    "\n",
    "# Returns the number of available CUDA-enabled GPUs\n",
    "print('Number of GPUs compatible with CUDA:' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Returns the name of the GPU at index 0\n",
    "print('Name of the GPU at index 0: '  + str(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# Returns the index of the current CUDA device being used\n",
    "print('Current CUDA device index: '  + str(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file_names_with_prefix\n",
    "# ファイル名の先頭部分（prefix）により自動的にファイル名を抽出するアルゴリズム。\n",
    "# 実際それぞれのファイル名は違うと思うので、必須ではない\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Extracts filenames in directory if they start with the prefix input \n",
    "\n",
    "\n",
    "Args/Parameters:\n",
    "\n",
    "    directory_path (string): The path of the dir (ex: /root/home/Documents/etc)\n",
    "    \n",
    "    prefix (string): Prefix of the file name (ex: 'Bo' is a prefix of 'Bone')\n",
    "\n",
    "Returns:\n",
    "\n",
    "    sorted_file_names (list of str): File names sorted in ascending order in the dir without extension ex: ['bone1', 'bone2', ...]\n",
    "\n",
    "Raises:\n",
    "\n",
    "    SomeError: ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def file_names_with_prefix(directory_path, prefix):\n",
    "\n",
    "    # Initialize an empty list to store the file names without extensions\n",
    "    file_names_without_extension = []\n",
    "\n",
    "    # Loop through all files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        #Checking if the file in loop exists in the directory_path not sure how is this necessary\n",
    "        #??\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            # Check if the file name starts with the specified prefix\n",
    "            if filename.startswith(prefix):\n",
    "                # Get the file name without extension\n",
    "                name_without_extension, _ = os.path.splitext(filename)\n",
    "\n",
    "                # Append the file name (without extension) to the list\n",
    "                file_names_without_extension.append(name_without_extension)\n",
    "\n",
    "    # Sort the list of file names without extensions in ascending order\n",
    "    sorted_file_names = sorted(\n",
    "        file_names_without_extension,\n",
    "        key=lambda x: (x.split('-')[0], int(x.split('-')[1]))\n",
    "    )  # Modify this part based on your file naming convention\n",
    "\n",
    "    # Now you have a sorted list of file names with the specified prefix and without extensions\n",
    "    return sorted_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory: /home/eric/Documents/cervicalResearchIIP\n",
      "Data directory (original dir): /home/eric/Documents/cervicalResearchIIP/img_1006t/original\n",
      "Feature img directory: /home/eric/Documents/cervicalResearchIIP/img_1006t/feature\n",
      "Labeled img directory: /home/eric/Documents/cervicalResearchIIP/img_1006t/labeled\n",
      "Annealing directory: /home/eric/Documents/cervicalResearchIIP/img_1006/original\n",
      "Result directory: /home/eric/Documents/cervicalResearchIIP/result/2025-06-23-MCU-Net-108-l6\n",
      "Test result directory: /home/eric/Documents/cervicalResearchIIP/result_test/2025-06-23-MCU-Net-108-l6\n"
     ]
    }
   ],
   "source": [
    "## paths config\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ここで、folder名とかPathとか色々設定\n",
    "\n",
    "# Setting the directory name, path and other settings\n",
    "\n",
    "# Define the root directory where your project is located\n",
    "# Defining a Path object for the project's root dir\n",
    "root_dir = Path(pathlib.Path.cwd())\n",
    "\n",
    "# result folder name\n",
    "#date_str = '20241202-Conv1x1-' + str(OUTPUT_DIR)\n",
    "date_str = OUTPUT_DIR\n",
    "\n",
    "# Define the directories for different types of data\n",
    "# Concatenating the root dir to the different dataset dirs\n",
    "data_dir = str(root_dir / \"img_1006t/original\")\n",
    "feature_dir = str(root_dir / \"img_1006t/feature\") \n",
    "labeled_dir = str(root_dir / \"img_1006t/labeled\")\n",
    "\n",
    "augmented_labeled_dir = str(root_dir / \"img_1006t/labelAug\")\n",
    "augmented_data_dir = str(root_dir / \"img_1006t/originalAug\")\n",
    "augmented_feature_dir = str(root_dir / \"img_1006t/featureAug\")\n",
    "\n",
    "\n",
    "# data_dir = str(root_dir / \"img_1006t/MC3x3(108_3out)/mcOriginal\")\n",
    "# feature_dir = str(root_dir / \"img_1006t/MC3x3(108_3out)/mcFeature\") \n",
    "# labeled_dir = str(root_dir / \"img_1006t/labeled\")\n",
    "\n",
    "# augmented_labeled_dir = str(root_dir / \"img_1006t/MC3x3(108_3out)/augMCLabel\")\n",
    "# augmented_data_dir = str(root_dir / \"img_1006t/MC3x3(108_3out)/augMCOriginal\")\n",
    "# augmented_feature_dir = str(root_dir / \"img_1006t/MC3x3(108_3out)/augMCFeature\")\n",
    "\n",
    "# annealing_img_dir = str(root_dir / \"img_1006/annealing_img\") # 焼きなまし法時に使う\n",
    "# annealing later, original for now\n",
    "annealing_img_dir = str(root_dir / \"img_1006/original\")\n",
    "result_dir = str(root_dir / \"result\" / date_str)\n",
    "test_result_dir= str(root_dir / \"result_test\" / date_str)\n",
    "\n",
    "# Making directories based on the path string result_dir and test_result_dir\n",
    "Path(result_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prints the paths of the dirs\n",
    "print('Root directory: ' + str(root_dir))\n",
    "print('Data directory (original dir): ' + str(data_dir))\n",
    "print('Feature img directory: ' + str(feature_dir))\n",
    "print('Labeled img directory: ' + str(labeled_dir))\n",
    "print('Annealing directory: ' + str(annealing_img_dir))\n",
    "print('Result directory: ' + str(result_dir))\n",
    "print('Test result directory: ' + str(test_result_dir))\n",
    "\n",
    "# Defining variables filename list of path str starts with the prefix format\n",
    "# In this case: N1 and N3 is training data and N2 is validation data and N4 is a test data\n",
    "input_train = []\n",
    "input_name_val = []\n",
    "annealing_input_name = []\n",
    "input_train = []\n",
    "test_input_name = []\n",
    "\n",
    "\n",
    "# for raw_input_img in INPUT:\n",
    "#     input_train.extend(file_names_with_prefix(data_dir, raw_input_img))\n",
    "# for raw_val_img in VALIDATION:\n",
    "#     input_name_val.extend(file_names_with_prefix(data_dir, raw_val_img))\n",
    "# for raw_anneal in ANNEALING:\n",
    "#     annealing_input_name.extend(file_names_with_prefix(data_dir, raw_anneal))\n",
    "# for raw_test in TEST:\n",
    "#     test_input_name.extend(file_names_with_prefix(data_dir, raw_test))\n",
    "\n",
    "\n",
    "####old version of assigning\n",
    "# input_train = file_names_with_prefix(data_dir, INPUT)\n",
    "# input_name_val = file_names_with_prefix(data_dir, VALIDATION)\n",
    "# annealing_input_name = file_names_with_prefix(data_dir, ANNEALING)\n",
    "# test_input_name = file_names_with_prefix(data_dir, TEST) \n",
    "\n",
    "\n",
    "# extra_dataset = file_names_with_prefix(data_dir,'N5-')\n",
    "# input_train.extend(extra_dataset)\n",
    "\n",
    "# Prints the each data image name\n",
    "# print(input_train)\n",
    "# print(input_name_val)\n",
    "# print(annealing_input_name)\n",
    "# print(test_input_name)\n",
    "# print(extra_dataset)\n",
    "\n",
    "\n",
    "# Defining a var to store each list length\n",
    "len_train = len(input_train)\n",
    "len_val = len(input_name_val)\n",
    "len_test = len(test_input_name)\n",
    "len_annealing = len(annealing_input_name)\n",
    "\n",
    "\n",
    "# print(len(input_train))\n",
    "\n",
    "# print(len(input_name_val))\n",
    "# print(len(test_input_name))\n",
    "# print(len(annealing_input_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GRY_', 'NML1', 'NML2', 'NML3', 'TOP1', 'TOP2', 'TOP3', 'TOP4', 'SBLX', 'SBLY', 'SBLM', 'SBLD', 'SBL1', 'SBL2', 'SBL3', 'SBL4', 'LPL1', 'LPL2', 'MEA1', 'MEA2', 'GAU1', 'GAU2', 'MED1', 'MED2', 'LBP1', 'LBP2', 'LBP3', 'ETC1', 'ETC2', 'STC1', 'STC2', 'HGF_', 'NGP_', 'POS1', 'POS2', 'POS3', 'SOL_', 'EMB1', 'EMB2', 'EMB3', 'KNN1', 'KNN2', 'BLT1', 'BLT2', 'OOO_', 'CAN1', 'CAN2', 'CAN3', 'PRE1', 'PRE2', 'PRE3', 'PRE4', 'UNS1', 'UNS2', 'UNS3', 'UNS4', 'UNS5', 'FOU1', 'FOU2', 'FOU3', 'FOU4', 'ERO1', 'ERO2', 'ERO3', 'ERO4', 'ERO5', 'ERO6', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'CLO1', 'CLO2', 'CLO3', 'CLO4', 'CLO5', 'SCH1', 'SCH2', 'SCH3', 'SCH4', 'ROB1', 'ROB2', 'ROB3', 'ROB4', 'MIN1', 'MIN2', 'MIN3', 'MIN4', 'MAX1', 'MAX2', 'MAX3', 'MAX4', 'MRG1', 'MRG2', 'MRG3', 'MRG4', 'MRL1', 'MRL2', 'MRL3', 'MRL4', 'BTM1', 'BTM2', 'BTM3', 'BTM4', 'DST_', 'HOM_', 'RIC_']\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "## feature list definition\n",
    "# 特徴画像の特徴一覧をリストとして取得\n",
    "inputfeature_list = list(map(str, InputFeature))\n",
    "\n",
    "# inputfeature_list = [\"GRY_\", \"NML1\", \"NML2\", \"NML3\", \"NML1\", \"MEA1\",\n",
    "#                      \"MEA2\", \"MED1\", \"MED2\", \"NML1\", \"NGP_\", \"KNN1\",\n",
    "#                      \"KNN2\", \"BLT1\", \"BLT2\", \"UNS1\", \"UNS2\", \"UNS3\",\n",
    "#                      \"UNS4\", \"UNS5\", \"ERO1\", \"ERO2\", \"ERO3\", \"ERO4\",\n",
    "#                      \"ERO5\", \"ERO6\", \"OPN1\", \"OPN2\", \"OPN3\", \"OPN4\",\n",
    "#                      \"OPN5\", \"CLO1\", \"CLO2\", \"CLO3\", \"CLO4\", \"CLO5\",\n",
    "#                      \"MIN1\", \"MIN2\", \"MIN3\", \"MIN4\", \"MAX1\", \"MAX2\",\n",
    "#                      \"MAX3\", \"MAX4\", \"MRL1\", \"MRL2\", \"MRL3\", \"MRL4\",\n",
    "#                      \"RIC_\",\n",
    "#                      ]\n",
    "\n",
    "\n",
    "\n",
    "# unet only\n",
    "#inputfeature_list = inputfeature_list[:1]\n",
    "\n",
    "#inputfeature_list.clear()\n",
    "\n",
    "#inputfeature_list.append(\"RIC_\")\n",
    "#print(inputfeature_list)\n",
    "\n",
    "#inputfeature_list.remove(OUTPUT_DIR[-4:])\n",
    "# inputfeature_list.remove(\"TOP1\")\n",
    "# inputfeature_list.remove(\"TOP2\")\n",
    "# inputfeature_list.remove(\"TOP3\")\n",
    "# inputfeature_list.remove(\"TOP4\")\n",
    "\n",
    "# inputfeature_list.remove(\"SBLX\")\n",
    "# inputfeature_list.remove(\"SBLY\")\n",
    "# inputfeature_list.remove(\"SBLM\")\n",
    "# inputfeature_list.remove(\"SBLD\")\n",
    "# inputfeature_list.remove(\"SBL1\")\n",
    "# inputfeature_list.remove(\"SBL2\")\n",
    "# inputfeature_list.remove(\"SBL3\")\n",
    "# inputfeature_list.remove(\"SBL4\")\n",
    "\n",
    "# inputfeature_list.remove(\"LPL1\")\n",
    "# inputfeature_list.remove(\"LPL2\")\n",
    "\n",
    "# inputfeature_list.remove(\"LBP1\")\n",
    "# inputfeature_list.remove(\"LBP2\")\n",
    "# inputfeature_list.remove(\"LBP3\")\n",
    "\n",
    "# inputfeature_list.remove(\"SOL_\")\n",
    "# inputfeature_list.remove(\"OOO_\")\n",
    "\n",
    "# inputfeature_list.remove(\"CAN1\")\n",
    "# inputfeature_list.remove(\"CAN2\")\n",
    "# inputfeature_list.remove(\"CAN3\")\n",
    "\n",
    "\n",
    "# inputfeature_list.remove(\"FOU1\")\n",
    "# inputfeature_list.remove(\"FOU2\")\n",
    "# inputfeature_list.remove(\"FOU3\")\n",
    "# inputfeature_list.remove(\"FOU4\")\n",
    "\n",
    "# inputfeature_list.remove(\"SCH1\")\n",
    "# inputfeature_list.remove(\"SCH2\")\n",
    "# inputfeature_list.remove(\"SCH3\")\n",
    "# inputfeature_list.remove(\"SCH4\")\n",
    "\n",
    "# inputfeature_list.remove(\"ROB1\")\n",
    "# inputfeature_list.remove(\"ROB2\")\n",
    "# inputfeature_list.remove(\"ROB3\")\n",
    "# inputfeature_list.remove(\"ROB4\")\n",
    "\n",
    "# inputfeature_list.remove(\"MRG1\")\n",
    "# inputfeature_list.remove(\"MRG2\")\n",
    "# inputfeature_list.remove(\"MRG3\")\n",
    "# inputfeature_list.remove(\"MRG4\")\n",
    "\n",
    "# inputfeature_list.remove(\"BTM1\")\n",
    "# inputfeature_list.remove(\"BTM2\")\n",
    "# inputfeature_list.remove(\"BTM3\")\n",
    "# inputfeature_list.remove(\"BTM4\")\n",
    "\n",
    "# inputfeature_list.remove(\"DST_\")\n",
    "# inputfeature_list.remove(\"HOM_\")\n",
    "\n",
    "# for i in range(108):\n",
    "#     stckd_feature_name = \"NGP_\" + inputfeature_list[i]\n",
    "#     inputfeature_list.append(stckd_feature_name)\n",
    "\n",
    "# for input_feature in inputfeature_list[:]:\n",
    "#     if not input_feature.startswith(\"NGP\"):\n",
    "#         inputfeature_list.remove(input_feature)\n",
    "        \n",
    "\n",
    "\n",
    "print(inputfeature_list)\n",
    "feature_num = len(inputfeature_list)\n",
    "\n",
    "\n",
    "print(feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## createweightimage read by images\n",
    "# takes too much memory cuz it loads all images into np array at once\n",
    "\n",
    "\n",
    "## 重み計算なし\n",
    "def CreateWeightImage(input_number, augmentation=False):\n",
    "    print(\"Creating image arrays...\")\n",
    "    label_dataset = []\n",
    "    arrDataset = []\n",
    "    for i in input_number:\n",
    "        if augmentation:\n",
    "            label_path = os.path.join(augmented_labeled_dir, str(AUGMENTATION) + \"aug/\" , f\"{i}.png\")\n",
    "        else:\n",
    "            label_path = os.path.join(labeled_dir, f\"{i}.png\")\n",
    "        input_originallabel = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # commented the binary label because the project has more labels than 2 \n",
    "        #_, binary_label = cv2.threshold(input_originallabel, 0, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        ## This part was used for decreasing and increasing the label count when there was inconsistency with the label dataset\n",
    "        \n",
    "        # if len(np.unique(input_originallabel)) > 11:\n",
    "        #     print(\"Defected image detected (more labels):\" + f\"{i}.png\")\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     for y in range(256):\n",
    "        #         for x in range (256):\n",
    "        #             if (input_originallabel[y][x] == 11) or (input_originallabel[y][x] == 12):\n",
    "        #                 input_originallabel[y][x] = 0\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     cv2.imwrite('testtttt.png', input_originallabel)\n",
    "        # elif len(np.unique(input_originallabel)) < 11:\n",
    "        #     print(\"Defected image detected (Less labels):\" + f\"{i}.png\")\n",
    "        #     defected = f\"{i}_.png\"\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     path_to_mask = '/home/eric/Desktop/edit8label'\n",
    "\n",
    "        #     # 25->9 32->10\n",
    "        #     path_mask_abs = os.path.join(path_to_mask, defected)\n",
    "        #     mask = cv2.imread(path_mask_abs, cv2.IMREAD_GRAYSCALE)\n",
    "        #     for y in range(256):\n",
    "        #         for x in range (256):\n",
    "        #             if (input_originallabel[y][x] == 0) and (mask[y][x] == 25):\n",
    "        #                 input_originallabel[y][x] = 9\n",
    "        #             elif (input_originallabel[y][x] == 0) and (mask[y][x] == 32):\n",
    "        #                 input_originallabel[y][x] = 10\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     label_fixed_path = os.path.join(path_to_mask, f\"{i}_fixed.png\")\n",
    "        #     cv2.imwrite(label_fixed_path, input_originallabel)\n",
    "                    \n",
    "        label_dataset.append(input_originallabel)\n",
    "\n",
    "    print(\"Number of label images:\", len(label_dataset))\n",
    "\n",
    "    for i in input_number:\n",
    "        # changed this part from 100 to 256\n",
    "\n",
    "        dataset_img =  np.zeros((256, 256, feature_num), dtype=np.float32)\n",
    "            \n",
    "            \n",
    "        for m in range(feature_num):\n",
    "            if augmentation:\n",
    "                feature_img_path = os.path.join(augmented_feature_dir, str(AUGMENTATION) + \"aug/\" , str(i), f\"{inputfeature_list[m]}.png\")\n",
    "            else:\n",
    "                feature_img_path = os.path.join(feature_dir, str(i), f\"{inputfeature_list[m]}.png\")\n",
    "            input_featureimg = cv2.imread(feature_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            dataset_img[:, :, m] = input_featureimg\n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "        # after the loop the dataset_img size will be like: (256, 256, 108) and the tensor type is np array\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##this part is dividing the dataset to use multiple conv1x1 operation\n",
    "        #split_dataset_img = np.array_split(dataset_img, 4, axis = 2)\n",
    "        #post_conv = np.zeros((256, 256, 4), dtype=np.float32)\n",
    "        #concat_list = []\n",
    "        #for index, part in enumerate(split_dataset_img):\n",
    "            # after this operation shape will be from (256, 256, 27) to (1, 256, 256, 27)\n",
    "            #part = torch.tensor(part, dtype = torch.float32).unsqueeze(0)\n",
    "            # changed the shape for the conv again now it is (1, 27, 256, 256) (batch_size, channels, height, width)\n",
    "            #part = part.permute(0, 3, 1, 2)\n",
    "            #fusion = nn.Conv2d(in_channels = 27, out_channels = 1, kernel_size = 1, padding = 'same')\n",
    "            #output_conv = fusion(part)\n",
    "            #output_conv = output_conv.squeeze(0) #.permute(1, 2, 0) # now it is (256, 256, 1)\n",
    "            #output_conv = output_conv.cpu().detach().numpy()\n",
    "            #post_conv[:, :, index] = output_conv[0]\n",
    "            #print((output_conv).shape)\n",
    "            #concat_list.append(output_conv)\n",
    "            \n",
    "        #concat_output = torch.cat((concat_list[0], concat_list[1], concat_list[2], concat_list[3]), dim = 2)\n",
    "        ## convert the concat result tensor into numpy array and it has to be on cpu to do the operation\n",
    "        #print(np.unique(output_conv.detach()))\n",
    "        #concat_output = concat_output.detach()\n",
    "        #concat_output = concat_output.numpy()\n",
    "        \n",
    "        \n",
    "        #arrDataset = concat_list\n",
    "        arrDataset.append(dataset_img)\n",
    "\n",
    "    arrDataset = np.array(arrDataset)\n",
    "    print(\"Completed creating image arrays:\")\n",
    "    print(\"Dataset shape \", arrDataset.shape)\n",
    "    print(\"Label image shape \", np.shape(label_dataset))\n",
    "    print()\n",
    "\n",
    "    return arrDataset, label_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## createweightimagenew read paths only\n",
    "def CreateWeightImageNew(input_numbers, augmentation=False, mrcnn=False):\n",
    "    print(\"Creating image paths...\")\n",
    "    label_paths = []\n",
    "    feature_paths = []\n",
    "\n",
    "    for i in input_numbers:\n",
    "        if augmentation:\n",
    "            label_path = os.path.join(augmented_labeled_dir, str(AUGMENTATION) + \"aug/\", f\"{i}.png\")\n",
    "        else:\n",
    "            label_path = os.path.join(labeled_dir, f\"{i}.png\")\n",
    "        label_paths.append(label_path)\n",
    "\n",
    "        feature_img_paths = []\n",
    "        if mrcnn:\n",
    "            inputfeature_list = [inputfeature_list[0]]\n",
    "        for feature_name in inputfeature_list:\n",
    "            if augmentation:\n",
    "                feature_img_path = os.path.join(augmented_feature_dir, str(AUGMENTATION) + \"aug/\", str(i), f\"{feature_name}.png\")\n",
    "            else:\n",
    "                feature_img_path = os.path.join(feature_dir, str(i), f\"{feature_name}.png\")\n",
    "            feature_img_paths.append(feature_img_path)\n",
    "\n",
    "        feature_paths.append(feature_img_paths)\n",
    "\n",
    "    print(f\"Processed {len(label_paths)} label paths and {len(feature_paths)} feature paths.\")\n",
    "    return feature_paths, label_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create weight test example\n",
    "\n",
    "# input_dataset,label_dataset = CreateWeightImage(input_train)\n",
    "# input_dataset_val,label_dataset_val = CreateWeightImage(input_name_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## createweightimageforshow\n",
    "# # # 重みを基づいて、MC画像を生成する\n",
    "# # # function for showing MC image\n",
    "# # # gets 1d array as weight, input_number is image name, and index is directory name for the mc image to save\n",
    "# def CreateWeightImageforShow(weight, input_number, index):\n",
    "#     sum_weight = sum(weight)  # Calculate total weight\n",
    "\n",
    "#     label_dataset = []\n",
    "#     input_dataset = []\n",
    "#     dataset_original = []\n",
    "\n",
    "#     # Read label images\n",
    "#     for i in input_number:\n",
    "#         input_originallabel = cv2.imread(labeled_dir + \"/\" + str(i) + \".png\", flags=0)\n",
    "#         label_dataset.append(input_originallabel)\n",
    "\n",
    "#     print('Weight image for show, label length = ', len(label_dataset))\n",
    "\n",
    "#     # Create output directory\n",
    "#     os.makedirs(test_result_dir + \"/weightImage/\" + str(index), exist_ok=True)\n",
    "\n",
    "#     # Generate weighted images\n",
    "#     for i in input_number:\n",
    "#         # Create a blank image to store the weighted image, using float type for accumulation\n",
    "#         dataset_img = np.zeros((256, 256, 3), dtype=np.float32)\n",
    "#         input_originalimg = cv2.imread(data_dir + \"/\" + str(i) + \".png\")\n",
    "\n",
    "#         dataset_original.append(input_originalimg)\n",
    "#         for m in range(feature_num):\n",
    "#             input_featureimg = cv2.imread(feature_dir + \"/\" + str(i) + \"/\" + inputfeature_list[m] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "#             # Normalize the feature image\n",
    "#             normalized_feature_img = cv2.normalize(input_featureimg.astype(np.float32), None, 0, 1, cv2.NORM_MINMAX)\n",
    "#             # Accumulate the weighted feature image\n",
    "#             dataset_img += normalized_feature_img[:, :, None] * (weight[m] / sum_weight)  # Convert 2D array to 3D array\n",
    "\n",
    "#         # Normalize the accumulated image to the range 0-255\n",
    "#         dataset_img = cv2.normalize(dataset_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "#         output_img = dataset_img.astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "#         input_dataset.append(output_img)\n",
    "#         # Write to file\n",
    "#         cv2.imwrite(f\"{test_result_dir}/weightImage/{index}/{i}.png\", output_img)\n",
    "\n",
    "#     return input_dataset, label_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## value extraction for visual mc\n",
    "# # ## Netron app to extract the weight tensor\n",
    "# # ## if the conv is 1x1 the tensor has 45 values\n",
    "# # ## if the conv is 3x3 there are 9 x 45 values so the function averages 9 value into 1 and so on\n",
    "# def compute_mean_values(input_list):\n",
    "#     \"\"\"\n",
    "#     Compute mean of 3x3 blocks in each channel of the input list.\n",
    "    \n",
    "#     Args:\n",
    "#         input_list (list): A 4D list of shape (1, C, H, W).\n",
    "        \n",
    "#     Returns:\n",
    "#         list: A 1D list of length C containing the mean values for each channel.\n",
    "#     \"\"\"\n",
    "#     # Ensure the input is a 4D list\n",
    "#     if not isinstance(input_list, list) or len(input_list) != 1:\n",
    "#         raise ValueError(\"Expected input list of shape (1, C, H, W)\")\n",
    "    \n",
    "#     channels = input_list[0]  # Get the channels (C, H, W)\n",
    "    \n",
    "#     if not isinstance(channels, list) or not all(isinstance(channel, list) for channel in channels):\n",
    "#         raise ValueError(\"Each channel must be a list of 2D lists (H, W).\")\n",
    "    \n",
    "#     mean_values = []\n",
    "#     for channel in channels:\n",
    "#         if not all(isinstance(row, list) for row in channel):\n",
    "#             raise ValueError(\"Each channel must contain 2D lists.\")\n",
    "        \n",
    "#         # Flatten the 2D channel to compute the mean\n",
    "#         flattened = [value for row in channel for value in row]\n",
    "#         mean_values.append(sum(flattened) / len(flattened))\n",
    "    \n",
    "#     return mean_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mc image visualization example\n",
    "# ### 1X1-1029-23-n1-1\n",
    "# ### JUST SINGLE VARIATION\n",
    "# weight_tensor_1x1_1029_23_n1_1_example = [\n",
    "#     [\n",
    "#         [\n",
    "#             [\n",
    "#                 0.048620592802762985\n",
    "#                 ...\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# input_number = ['N1-1']\n",
    "\n",
    "# #single\n",
    "# weight_cn = compute_mean_values(weight_tensor_1x1_1029_23_n1_1)\n",
    "\n",
    "# weight_cn1 = compute_mean_values(weight_tensor_1x1_1029_36_n1_1)\n",
    "# weight_cn2 = compute_mean_values(weight_tensor_3x3_1_1029_36_n1_1)\n",
    "# weight_cn3 = compute_mean_values(weight_tensor_3x3_2_1029_36_n1_1)\n",
    "# weight_cn4 = compute_mean_values(weight_tensor_5x5_1_1029_36_n1_1)\n",
    "# weight_cn5 = compute_mean_values(weight_tensor_5x5_2_1029_36_n1_1)\n",
    "\n",
    "# print(len(weight_cn))\n",
    "# print(len(weight_cn1))\n",
    "# print(len(weight_cn2))\n",
    "# print(len(weight_cn3))\n",
    "# print(len(weight_cn4))\n",
    "# print(len(weight_cn5))\n",
    "\n",
    "# CreateWeightImageforShow(weight_cn, input_number, \"single\")\n",
    "\n",
    "# CreateWeightImageforShow(weight_cn1, input_number, \"multiple1x1\")\n",
    "# CreateWeightImageforShow(weight_cn2, input_number, \"multiple3x3-1\")\n",
    "# CreateWeightImageforShow(weight_cn3, input_number, \"multiple3x3-2\")\n",
    "# CreateWeightImageforShow(weight_cn4, input_number, \"multiple5x5-1\")\n",
    "# CreateWeightImageforShow(weight_cn5, input_number, \"multiple5x5-2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mc image visualization example\n",
    "# weight_tensor = np.load(\"/home/eric/Documents/cervicalResearchIIP/result_test/20250420-MCU-Net-108/0420_MCU108_fold4_w.npy\")\n",
    "\n",
    "# # initial shape (3, 108, 3, 3) (out_channels, channels, height_convolution, width_convolution)\n",
    "# # np.mean takes mean value of out_channels\n",
    "# # [np.newaxis, :] shapes into (1, 108, 3, 3)\n",
    "# weight_tensor = np.mean(weight_tensor, axis = 0)[np.newaxis, :]\n",
    "# weight_tensor = weight_tensor.tolist()\n",
    "\n",
    "# #f1\n",
    "# #input_number = [\"N1-1\", \"N1-7\", \"N1-9\", \"N2-3\", \"N2-7\", \"N2-10\", \"N3-1\", \"N3-8\", \"N3-10\", \"N4-8\", \"N5-4\"]\n",
    "# #f2\n",
    "# #input_number = [\"N1-2\", \"N1-5\", \"N1-10\", \"N2-4\", \"N3-2\", \"N3-4\", \"N3-5\", \"N3-9\", \"N4-3\", \"N4-9\", \"N5-2\"]\n",
    "# #f3\n",
    "# #input_number = [\"N1-3\", \"N1-4\", \"N1-10\", \"N2-1\", \"N2-5\", \"N2-8\", \"N3-3\", \"N3-7\", \"N4-4\", \"N4-5\", \"N4-6\", \"N5-1\", \"N5-6\"]\n",
    "# #f4\n",
    "# input_number = [\"N1-6\", \"N1-8\", \"N2-2\", \"N2-6\", \"N2-9\", \"N3-6\", \"N4-1\", \"N4-2\", \"N4-7\", \"N5-3\", \"N5-5\"]\n",
    "\n",
    "# # weight_tensor_converted = weight_tensor[0].tolist()\n",
    "\n",
    "# # weight_tensor_dv1 = [weight_tensor_converted]\n",
    "# # # print(weight_tensor_dv1.shape)\n",
    "# weight_cn  = compute_mean_values(weight_tensor)\n",
    "\n",
    "# CreateWeightImageforShow(weight_cn, input_number, \"fold4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## minor irrelevant function\n",
    "def print_model_shapes(model, input_tensor):\n",
    "    def forward_hook(module, input, output):\n",
    "        print(f\"Layer: {module.__class__.__name__}\")\n",
    "        print(f\"Input shape: {str(input[0].shape)}\")\n",
    "        print(f\"Output shape: {str(output.shape)}\")\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "    hooks = []\n",
    "    for layer in model.children():\n",
    "        hook = layer.register_forward_hook(forward_hook)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    # Pass a dummy input tensor through the model to trigger the forward hooks\n",
    "    with torch.no_grad():\n",
    "        model(input_tensor)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess and postprocess function\n",
    "def preprocess(img: np.ndarray):\n",
    "    img = np.moveaxis(img, -1, 0)  # Change from [H, W, C] to [C, H, W]\n",
    "    img = normalize_01(img)  # Linear scaling to range [0-1]\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension [B, C, H, W]\n",
    "    img = img.astype(np.float32)  # Typecasting to float32\n",
    "    #print(\"in pre\")\n",
    "    #print(np.unique(img))\n",
    "    return img\n",
    "\n",
    "# postprocess function\n",
    "def postprocess(img: torch.tensor):\n",
    "    img = torch.argmax(img, dim = 1)  # Perform argmax to generate 1 channel\n",
    "    #img = img * 255.0 commented as the labels are from 0 to 11 in my case\n",
    "    img = img.cpu().numpy().astype(np.uint8)  # Send to CPU and transform to numpy.ndarray\n",
    "    # If batch_size > 1, you may need to loop through each batch and save them separately\n",
    "    # If batch_size == 1, you can remove the batch dimension to save a single image\n",
    "\n",
    "    # used for checking the unique label values whether if it is 0 to 11 or 0 to 255 scale\n",
    "    #print(\"in post\")\n",
    "    #print(np.unique(img))\n",
    "\n",
    "\n",
    "    img = np.squeeze(img)  # Remove batch dim and channel dim -> [H, W]\n",
    "    # img = re_normalize(img)  # Scale it to the range [0-255]\n",
    "\n",
    "    # If your image has multiple channels (C>1), like an RGB image, before saving with cv2.imwrite\n",
    "    # you need to ensure the channel order is [B, G, R] instead of the common [R, G, B]\n",
    "    # If C == 1, you can further reduce dimensions -> [H, W]\n",
    "    if img.shape[0] == 3:  # [C, H, W]\n",
    "        img = np.transpose(img, (1, 2, 0))  # [H, W, C]\n",
    "        img = img[:, :, ::-1]  # Convert RGB to BGR\n",
    "    elif img.shape[0] == 1:  # [C, H, W]\n",
    "        img = np.squeeze(img, 0)  # [H, W]\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learn_ea function training logic\n",
    "# earlystoppingあり\n",
    "# numpy形式のまま入力する用改良\n",
    "# 学習を行い予測結果画像を出力するとこまで\n",
    "from customdatasets import SegmentationDataSet0\n",
    "from customdatasets import SegmentationDataSet1\n",
    "from customdatasets import SegmentationDataSet5\n",
    "from customdatasets import SegmentationDataSetMaskRcnn\n",
    "from torch.utils.data import DataLoader\n",
    "from maskrcnn import get_maskrcnn_model\n",
    "\n",
    "def Learn_EA(input_dataset,label_dataset,input_dataset_val,label_dataset_val, type_number, fold=0):\n",
    "    print(\"*************************Training*************************\")\n",
    "    # 引数を追加して保存先を指定するよう改良\n",
    "    # try_number:何回目の焼きなましかどうか。モデルの保存に使用\n",
    "\n",
    "    # dataset training\n",
    "    # dataset_train2 = SegmentationDataSet0(\n",
    "    #                                     #inputs=dataset_original,\n",
    "    #                                     inputs=input_dataset,\n",
    "    #                                     targets=label_dataset,\n",
    "    #                                     transform=transforms_training)\n",
    "    \n",
    "    dataset_train2 = SegmentationDataSet5(\n",
    "                                        #inputs=dataset_original,\n",
    "                                        feature_paths=input_dataset,\n",
    "                                        label_paths=label_dataset,\n",
    "                                        feature_num = IN_CHANNEL,\n",
    "                                        transform=transforms_training)\n",
    "\n",
    "    # dataloader training\n",
    "    #rearranged in custom order so shuffle is false in normal case: true\n",
    "    dataloader_training2 = DataLoader(dataset=dataset_train2,\n",
    "                                     batch_size = 2,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=4,\n",
    "                                     pin_memory=True)\n",
    "    #もとはシャッフルtrue\n",
    "\n",
    "\n",
    "    batch = next(iter(dataloader_training2))\n",
    "  \n",
    "    x, y = batch\n",
    "    print(\"x.shape = \", x.shape)\n",
    "    print(\"x.min(), x.max() = \", x.min(), x.max())\n",
    "    print(\"y.shape = \", y.shape)\n",
    "    print(\"torch.unique(y) = \", torch.unique(y))\n",
    "\n",
    "\n",
    "    \n",
    "    # dataset training\n",
    "    dataset_val = SegmentationDataSet0(inputs=input_dataset_val,\n",
    "                                        targets=label_dataset_val,\n",
    "                                        transform=transforms_val)\n",
    "    #書き換え箇所\n",
    "    dataloader_val = DataLoader(dataset=dataset_val,\n",
    "                                     batch_size = 2,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=4,\n",
    "                                     pin_memory=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###earlystopping あり\n",
    "\n",
    "    from unet import UNet\n",
    "    from trainer2 import Trainer2 \n",
    "    from torch import nn #import torch \n",
    "    from pytorchtools import EarlyStopping\n",
    "    from torch.nn import BCEWithLogitsLoss\n",
    "    from customLoss import DiceCELoss\n",
    "    from customLoss import DiceLoss\n",
    "    from customLoss import ExponentialLogCE_DiceLoss\n",
    "\n",
    "\n",
    "    #device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda') \n",
    "    else: \n",
    "        torch.device('cpu')\n",
    "        print(\"Before creating the UNet model: GPU was not available and CPU will be used instead\")\n",
    "\n",
    "    # custom logging the parameters of the UNet\n",
    "    inChannels = IN_CHANNEL\n",
    "    outChannels = 11\n",
    "    nBlocks = N_BLOCK\n",
    "    startFilters = 32\n",
    "\n",
    "    from customLog import custom_logger\n",
    "    \n",
    "    #custom_logger(\"/log/customLog.log\", inChannels, outChannels, nBlocks, startFilters)\n",
    "\n",
    "    #model\n",
    "    model = UNet(in_channels = inChannels,\n",
    "                 out_channels = outChannels,\n",
    "                 n_blocks = nBlocks, \n",
    "                 start_filters=32,\n",
    "                 activation='relu',\n",
    "                 normalization='instance',\n",
    "                 conv_mode='same',\n",
    "                 dim=2,\n",
    "                 ).to(device)\n",
    "\n",
    "    ### Later for experimenting\n",
    "    #from originalUNet import Original_UNet\n",
    "    #model = Original_UNet()\n",
    "\n",
    "    # Assuming input_tensor is a sample input tensor with the correct shape (e.g., torch.randn(1, 3, 100, 100))\n",
    "    input_tensor = torch.randn(1, 45, 100, 100).to(device)  # Adjust the shape as needed\n",
    "    # print_model_shapes(model, input_tensor)\n",
    "\n",
    "\n",
    "    #loss function\n",
    "    \n",
    "    # pytorch cross entropy loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #custom dice loss\n",
    "    #criterion = DiceLoss()\n",
    "    \n",
    "    #custom combination loss of dice and cross entropy\n",
    "    # 40 60 ratio\n",
    "    #criterion = DiceCELoss(dice_weight = DICE_IMPORTANCE, ce_weight = CE_IMPORTANCE)\n",
    "    \n",
    "    # pytorch bce loss \n",
    "    # criterion = BCEWithLogitsLoss()\n",
    "    \n",
    "    # custom combination log exp loss of dice and cross entropy\n",
    "    # requires segmentation class number\n",
    "    #criterion = ExponentialLogCE_DiceLoss(num_class = 11)\n",
    "\n",
    "    #optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    #trainer\n",
    "    trainer = Trainer2(model=model, \n",
    "                       device=device, \n",
    "                       criterion=criterion, \n",
    "                       optimizer=optimizer, \n",
    "                       training_DataLoader=dataloader_training2,\n",
    "                       #validation_DataLoader=None, \n",
    "                       validation_DataLoader=dataloader_val, \n",
    "                       lr_scheduler=None, \n",
    "                       epochs=146, ##😺😺😺😺 epoch=0, \n",
    "                       notebook=True)\n",
    "  \n",
    "    print(\"=======start training======\")\n",
    "    \n",
    "    # start training\n",
    "    training_losses, validation_losses, lr_rates = trainer.run_trainer()\n",
    "    print(\"***************************\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #ここがちゃんとESで最適なエポック数のモデルになっているか要検証\n",
    "    model_dir = os.path.join(\"model\", date_str)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_name = f\"model_fold_{fold}.pt\"\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"modelname:{model_name}を保存しました\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    model_weights = torch.load(model_path)\n",
    "    model.load_state_dict(model_weights)\n",
    "    \n",
    "    \n",
    "\n",
    "    # images = annealing_input_dataset\n",
    "    images = input_dataset_val\n",
    "\n",
    "    from inference import predict\n",
    "    from transformations import normalize_01, re_normalize\n",
    "    # predict the segmentation maps \n",
    "    output = [predict(img, model, preprocess, postprocess, device) for img in images]\n",
    "\n",
    "\n",
    "    for i in range(len(input_name_val)):\n",
    "        if(type_number == 0):\n",
    "            cv2.imwrite(os.path.join(result_dir, f'{input_name_val[i]}.png'), output[i])\n",
    "        elif(type_number == 1):\n",
    "            cv2.imwrite(os.path.join(test_result_dir, 'result_original', f'{input_name_val[i]}.png'), output[i])\n",
    "        elif(type_number == 2):\n",
    "            cv2.imwrite(os.path.join(test_result_dir, 'result_test', f'{input_name_val[i]}.png'), output[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learn_ea for mask rcnn function training logic\n",
    "# earlystoppingあり\n",
    "# numpy形式のまま入力する用改良\n",
    "# 学習を行い予測結果画像を出力するとこまで\n",
    "from customdatasets import SegmentationDataSet0\n",
    "from customdatasets import SegmentationDataSet1\n",
    "from customdatasets import SegmentationDataSet5\n",
    "from customdatasets import SegmentationDataSetMaskRcnn\n",
    "from torch.utils.data import DataLoader\n",
    "from maskrcnn import get_maskrcnn_model\n",
    "from maskrcnn import Trainer2MaskRCNN\n",
    "from inference import predict_mrcnn\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def Learn_EA_mask_rcnn(input_dataset,label_dataset,input_dataset_val,label_dataset_val, type_number, fold=0):\n",
    "    print(\"*************************Training*************************\")\n",
    "    \n",
    "    # ****************maskrcnn train variable****************\n",
    "    #\n",
    "    \n",
    "    dataset_train_mrcnn = SegmentationDataSetMaskRcnn(\n",
    "                                        input_paths = input_dataset,\n",
    "                                        label_paths=label_dataset,\n",
    "                                        transform=transforms_training)\n",
    "\n",
    "\n",
    "    \n",
    "    dataloader_training2 = DataLoader(dataset=dataset_train_mrcnn,\n",
    "                                     batch_size = 2,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=4,\n",
    "                                     pin_memory=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "    \n",
    "    #\n",
    "    # ****************maskrcnn train variable****************\n",
    "\n",
    "    batch = next(iter(dataloader_training2))\n",
    "  \n",
    "    x, y = batch\n",
    "        \n",
    "    print(\"len(x) =\", len(x))  # e.g., 2 if batch_size=2\n",
    "    print(\"x[0].shape =\", x[0].shape)\n",
    "    print(\"x[0].min(), x[0].max() =\", x[0].min(), x[0].max())\n",
    "    # print(\"x.shape = \", x.shape)\n",
    "    # print(\"x.min(), x.max() = \", x.min(), x.max())\n",
    "    # print(\"y.shape = \", y.shape)\n",
    "    # print(\"torch.unique(y) = \", torch.unique(y))\n",
    "\n",
    "\n",
    "    \n",
    "    # dataset training\n",
    "    dataset_val = SegmentationDataSetMaskRcnn(\n",
    "                                        input_paths=input_dataset_val,\n",
    "                                        label_paths=label_dataset_val,\n",
    "                                        transform=transforms_training)\n",
    "    #書き換え箇所\n",
    "    dataloader_val = DataLoader(dataset=dataset_val,\n",
    "                                     batch_size = 2,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=4,\n",
    "                                     pin_memory=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###earlystopping あり\n",
    "\n",
    "    from unet import UNet\n",
    "    from trainer2 import Trainer2 \n",
    "    from torch import nn #import torch \n",
    "    from pytorchtools import EarlyStopping\n",
    "    from torch.nn import BCEWithLogitsLoss\n",
    "    from customLoss import DiceCELoss\n",
    "    from customLoss import DiceLoss\n",
    "    from customLoss import ExponentialLogCE_DiceLoss\n",
    "\n",
    "\n",
    "    #device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda') \n",
    "    else: \n",
    "        torch.device('cpu')\n",
    "        print(\"Before creating the UNet model: GPU was not available and CPU will be used instead\")\n",
    "\n",
    "\n",
    "    from customLog import custom_logger\n",
    "    \n",
    "    #custom_logger(\"/log/customLog.log\", inChannels, outChannels, nBlocks, startFilters)\n",
    "\n",
    "    #model\n",
    "    model = get_maskrcnn_model(num_classes=11).to(device)\n",
    "\n",
    "    ### Later for experimenting\n",
    "    #from originalUNet import Original_UNet\n",
    "    #model = Original_UNet()\n",
    "\n",
    "    # Assuming input_tensor is a sample input tensor with the correct shape (e.g., torch.randn(1, 3, 100, 100))\n",
    "    input_tensor = torch.randn(1, 45, 100, 100).to(device)  # Adjust the shape as needed\n",
    "    # print_model_shapes(model, input_tensor)\n",
    "\n",
    "\n",
    "    #loss function\n",
    "    \n",
    "    # pytorch cross entropy loss function\n",
    "    #criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #custom dice loss\n",
    "    #criterion = DiceLoss()\n",
    "    \n",
    "    #custom combination loss of dice and cross entropy\n",
    "    # 40 60 ratio\n",
    "    #criterion = DiceCELoss(dice_weight = DICE_IMPORTANCE, ce_weight = CE_IMPORTANCE)\n",
    "    \n",
    "    # pytorch bce loss \n",
    "    # criterion = BCEWithLogitsLoss()\n",
    "    \n",
    "    # custom combination log exp loss of dice and cross entropy\n",
    "    # requires segmentation class number\n",
    "    #criterion = ExponentialLogCE_DiceLoss(num_class = 11)\n",
    "\n",
    "    #optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-7)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "    #trainer\n",
    "    trainer = Trainer2MaskRCNN(model=model, \n",
    "                       device=device, \n",
    "                       #criterion=criterion, \n",
    "                       optimizer=optimizer, \n",
    "                       training_DataLoader=dataloader_training2,\n",
    "                       #validation_DataLoader=None, \n",
    "                       validation_DataLoader=dataloader_val, \n",
    "                       epochs=50, ##😺😺😺😺 epoch=0, \n",
    "                       )\n",
    "  \n",
    "    print(\"=======start training======\")\n",
    "    \n",
    "    # start training\n",
    "    trainer.run_trainer()\n",
    "    print(\"***************************\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #ここがちゃんとESで最適なエポック数のモデルになっているか要検証\n",
    "    model_dir = os.path.join(\"model\", date_str)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_name = f\"model_mask_rcnn_fold_{fold}.pt\"\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"modelname:{model_name}を保存しました\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    clear_cuda_memory()\n",
    "\n",
    "    model_weights = torch.load(model_path)\n",
    "    model.load_state_dict(model_weights)\n",
    "    \n",
    "    \n",
    "\n",
    "    # images = annealing_input_dataset\n",
    "    outputs = []\n",
    "    \n",
    "    for img_path in label_dataset_val:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        # plt.show()\n",
    "        img = (img > 0).astype(np.uint8)\n",
    "        # plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "        # plt.axis('off')  # Hide axes\n",
    "        # plt.show()\n",
    "        output = predict_mrcnn(img, model, preprocess, postprocess, device)\n",
    "        outputs.append(output[0])\n",
    "\n",
    "    from inference import predict\n",
    "    from transformations import normalize_01, re_normalize\n",
    "    # predict the segmentation maps \n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(input_name_val)):\n",
    "        if(type_number == 0):\n",
    "            val_path = result_dir + f'/Mask_RCNN_{input_name_val[i]}.png'\n",
    "            cv2.imwrite(val_path, outputs[i])\n",
    "        elif(type_number == 1):\n",
    "            cv2.imwrite(os.path.join(test_result_dir, 'result_original', f'Mask_RCNN_{input_name_val[i]}.png'), outputs[i])\n",
    "        elif(type_number == 2):\n",
    "            cv2.imwrite(os.path.join(test_result_dir, 'result_test', f'Mask_RCNN_{input_name_val[i]}.png'), outputs[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dice calculation\n",
    "import statistics\n",
    "# Dicecの計算\n",
    "def cal_DiceMulitple(dir, input_name, mrcnn=False):\n",
    "    # change the label count as your preference\n",
    "    Dice = [0] * 11\n",
    "    Count1 = [0] * 11 #予測結果の各ラベルの要素数\n",
    "    Count2 = [0] * 11 #ラベル画像の\n",
    "    Count3 = [0] * 11 #正解した画素数\n",
    "    \n",
    "    for index in range(len(input_name)):\n",
    "        print('index = ', index)\n",
    "\n",
    "        if mrcnn == True:\n",
    "            img1 = cv2.imread(dir + '/Mask_RCNN_' + input_name[index] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        else:\n",
    "            img1 = cv2.imread(dir + '/' + input_name[index] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(labeled_dir + '/' + input_name[index] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        #_, img2 = cv2.threshold(img2, 0, 255, cv2.THRESH_BINARY)\n",
    "        print(\"予測画像:\", dir  + '/' + input_name[index] + \".png\")\n",
    "        print(\"テストラベル:\", labeled_dir + '/' + input_name[index] + \".png\")\n",
    "        unique_label1 = np.unique(img1)\n",
    "        unique_label2 = np.unique(img2)\n",
    "        # print(unique_label1)\n",
    "        # print(unique_label2)\n",
    "        # change the image array size to your need\n",
    "        for n in range(256):\n",
    "            for l in range(256):\n",
    "                value1 = img1[n,l]\n",
    "                # for index, uq_value in enumerate(unique_label1):\n",
    "                #     if(value1 == uq_value):\n",
    "                #         value1 = index\n",
    "\n",
    "                Count1[value1] += 1\n",
    "\n",
    "                value2 = img2[n,l]\n",
    "                Count2[value2] += 1                    \n",
    "\n",
    "                if(value1 == value2):\n",
    "                    Count3[value1] += 1 \n",
    "    for i in range(11):\n",
    "        if(Count1[i]+Count2[i] != 0):\n",
    "            Dice[i] = (2*Count3[i])/(Count1[i] + Count2[i])\n",
    "        if(Count1[i]+Count2[i] == 0):\n",
    "            print(\"\")\n",
    "            #print(\"4 label case:\" + str(input_name[index]))\n",
    "    Dice.append(statistics.mean(Dice[1:]))\n",
    "    print('Count1 = ', Count1)\n",
    "    print('Count2 = ', Count2)\n",
    "    print('Count3 = ', Count3)\n",
    "    print('Dice = ', Dice)\n",
    "    #print(unique_label)\n",
    "\n",
    "    return Dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mConv_predict test function\n",
    "import statistics\n",
    "from denseCRF import noiseReduction\n",
    "# device\n",
    "def mConv_predict(test_input_name, fold=0):\n",
    "    print(\"*************************************Test*************************************\")\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"Using CPU instead of GPU\")\n",
    "        torch.device('cpu')\n",
    "\n",
    "    model = UNet(in_channels=IN_CHANNEL,  # check the parameters\n",
    "                out_channels=11,\n",
    "                n_blocks=N_BLOCK,\n",
    "                start_filters=32,\n",
    "                activation='relu',\n",
    "                normalization='instance', #use instance when \"batch\" size is less than 10? batch\n",
    "                conv_mode='same',\n",
    "                dim=2).to(device)\n",
    "\n",
    "    model_dir = os.path.join(\"model\", date_str)\n",
    "    print(\"model_dir = \", model_dir)\n",
    "    model_path = os.path.join(model_dir, f\"model_fold_{fold}.pt\") # load the model\n",
    "    model_weights = torch.load(model_path)\n",
    "    model.load_state_dict(model_weights)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # create test dataset\n",
    "    test_input_dataset, _ = CreateWeightImage(test_input_name)\n",
    " \n",
    "\n",
    "    # convert to torch\n",
    "    test_input_tensor = torch.from_numpy(test_input_dataset).float().to(device)\n",
    "\n",
    "\n",
    "    from inference import predict\n",
    "    from transformations import normalize_01, re_normalize\n",
    "    # predict\n",
    "    output = [predict(img, model, preprocess, postprocess, device) for img in test_input_dataset]\n",
    "\n",
    "\n",
    "    plot_output_img = list()\n",
    "    \n",
    "    \n",
    "    fold_dir = os.path.join(test_result_dir, f\"fold{fold}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    # save images\n",
    "    for i, output_img in enumerate(output):\n",
    "        #for checking the unique values of the result label\n",
    "        labels = np.unique(output_img)\n",
    "\n",
    "        output_image_path = os.path.join(fold_dir, f\"{test_input_name[i]}.png\")\n",
    "        cv2.imwrite(output_image_path, output_img)\n",
    "        \n",
    "        # old writing when there was no stratifiedkfold\n",
    "        # cv2.imwrite(os.path.join(test_result_dir, f\"{test_input_name[i]}.png\"), output_img)  # 根据需要调整文件路径和命名\n",
    "\n",
    "        #print(labels)\n",
    "        plot_output_img.append(output_img)\n",
    "\n",
    "\n",
    "    Dice = cal_DiceMulitple(fold_dir, test_input_name)\n",
    "    df = pd.DataFrame(Dice)\n",
    "    df = df.T\n",
    "    df.to_csv(test_result_dir + \"/Dice.csv\", mode='a', header=False) # Diceの結果をcsvに追加\n",
    "    \n",
    "    postCRF = noiseReduction(output, test_input_name, labeled_dir, fold_dir, 0.8)\n",
    "    \n",
    "    DiceCRF = cal_DiceMulitple( fold_dir + \"/crf\", test_input_name)\n",
    "    dfCRF = pd.DataFrame(DiceCRF)\n",
    "    dfCRF = dfCRF.T\n",
    "    dfCRF.to_csv(test_result_dir + \"/CRFDice.csv\", mode='a', header=False) # Diceの結果をcsvに追加\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mConv_predict test function\n",
    "import statistics\n",
    "from denseCRF import noiseReduction\n",
    "from maskrcnn import get_maskrcnn_model\n",
    "# device\n",
    "def mConv_predict_mask_rcnn(test_input_name, fold=0):\n",
    "    print(\"*************************************Test*************************************\")\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"Using CPU instead of GPU\")\n",
    "        torch.device('cpu')\n",
    "\n",
    "    model = get_maskrcnn_model(11).to(device)\n",
    "\n",
    "    model_dir = os.path.join(\"model\", date_str)\n",
    "    print(\"model_dir = \", model_dir)\n",
    "    model_path = os.path.join(model_dir, f\"model_mask_rcnn_fold_{fold}.pt\") # load the model\n",
    "    model_weights = torch.load(model_path)\n",
    "    model.load_state_dict(model_weights)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # create test dataset\n",
    "    #_, test_input_dataset = CreateWeightImage(test_input_name)\n",
    "    # convert binary for mrcnn\n",
    "    #test_input_dataset = [(img > 0).astype(np.uint8) for img in test_input_dataset]\n",
    "    # images_test = []\n",
    "    # for img in test_input_dataset:\n",
    "    #     img = (img > 0).astype(np.uint8)\n",
    "    #     output = predict_mrcnn(img, model, preprocess, postprocess, device)\n",
    "    #     images_test.append(output[0])\n",
    "    # print(type(images_test))\n",
    "    # convert to torch\n",
    "    #test_input_tensor = torch.from_numpy(test_input_dataset).float().to(device)\n",
    "    \n",
    "    # for unet based mrcnn\n",
    "    #fold_dir = os.path.join(test_result_dir, f\"fold{fold}/crf/\")\n",
    "    # for pure mrcnn\n",
    "    fold_dir = data_dir\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    images_test = []\n",
    "    for i, img in enumerate(test_input_name):\n",
    "        img_path = fold_dir + f\"{test_input_name[i]}.png\"\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # for unet based mrcnn\n",
    "        #img = (img > 0).astype(np.uint8)\n",
    "        output = predict_mrcnn(img, model, preprocess, postprocess, device)\n",
    "        images_test.append(output[0])\n",
    "    print(type(images_test))\n",
    "\n",
    "    from inference import predict\n",
    "    from transformations import normalize_01, re_normalize\n",
    "    # predict\n",
    "    # to be used in real unet combination\n",
    "    # outputs = []\n",
    "    \n",
    "    # for img_path in test_input_dataset:\n",
    "    #     img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    #     # plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "    #     # plt.axis('off')  # Hide axes\n",
    "    #     # plt.show()\n",
    "    #     img = (img > 0).astype(np.uint8)\n",
    "    #     # plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "    #     # plt.axis('off')  # Hide axes\n",
    "    #     # plt.show()\n",
    "    #     output = predict_mrcnn(img, model, preprocess, postprocess, device)\n",
    "    #     outputs.append(img[0])\n",
    "    \n",
    "\n",
    "\n",
    "    plot_output_img = list()\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    # save images\n",
    "    for i, output_img in enumerate(images_test):\n",
    "        #for checking the unique values of the result label\n",
    "        labels = np.unique(output_img)\n",
    "\n",
    "        output_image_path = os.path.join(fold_dir, f\"Mask_RCNN_{test_input_name[i]}.png\")\n",
    "        cv2.imwrite(output_image_path, output_img)\n",
    "        \n",
    "        # old writing when there was no stratifiedkfold\n",
    "        # cv2.imwrite(os.path.join(test_result_dir, f\"{test_input_name[i]}.png\"), output_img)  # 根据需要调整文件路径和命名\n",
    "\n",
    "        #print(labels)\n",
    "        plot_output_img.append(output_img)\n",
    "\n",
    "    Dice = cal_DiceMulitple(fold_dir, test_input_name, True)\n",
    "    df = pd.DataFrame(Dice)\n",
    "    df = df.T\n",
    "    df.to_csv(test_result_dir + \"/DiceMaskRCNN.csv\", mode='a', header=False) # Diceの結果をcsvに追加\n",
    "    \n",
    "    # postCRF = noiseReduction(output, test_input_name, labeled_dir, fold_dir, 0.8)\n",
    "    \n",
    "    # DiceCRF = cal_DiceMulitple( fold_dir + \"/crf\", test_input_name)\n",
    "    # dfCRF = pd.DataFrame(DiceCRF)\n",
    "    # dfCRF = dfCRF.T\n",
    "    # dfCRF.to_csv(test_result_dir + \"/CRFDice.csv\", mode='a', header=False) # Diceの結果をcsvに追加\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N1-1' 'N1-2' 'N1-3' 'N1-4' 'N1-5' 'N1-6' 'N1-7' 'N1-8' 'N1-9' 'N1-10'\n",
      " 'N2-1' 'N2-2' 'N2-3' 'N2-4' 'N2-5' 'N2-6' 'N2-7' 'N2-8' 'N2-9' 'N2-10'\n",
      " 'N3-1' 'N3-2' 'N3-3' 'N3-4' 'N3-5' 'N3-6' 'N3-7' 'N3-8' 'N3-9' 'N3-10'\n",
      " 'N4-1' 'N4-2' 'N4-3' 'N4-4' 'N4-5' 'N4-6' 'N4-7' 'N4-8' 'N4-9' 'N5-1'\n",
      " 'N5-2' 'N5-3' 'N5-4' 'N5-5' 'N5-6']\n",
      "['N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N2' 'N2' 'N2' 'N2'\n",
      " 'N2' 'N2' 'N2' 'N2' 'N2' 'N2' 'N3' 'N3' 'N3' 'N3' 'N3' 'N3' 'N3' 'N3'\n",
      " 'N3' 'N3' 'N4' 'N4' 'N4' 'N4' 'N4' 'N4' 'N4' 'N4' 'N4' 'N5' 'N5' 'N5'\n",
      " 'N5' 'N5' 'N5']\n",
      "Cross validation: True\n",
      "Fold: 2 out of 4\n",
      "Augmentation: True\n",
      "Augmentation amount: 30\n",
      "Training: Total of 22 cases.\n",
      "['N4-1' 'N3-1' 'N2-1' 'N3-4' 'N5-1' 'N2-6' 'N4-9' 'N1-5' 'N1-1' 'N1-2'\n",
      " 'N3-2' 'N2-4' 'N5-4' 'N2-2' 'N4-3' 'N1-10' 'N1-7' 'N3-9' 'N5-2' 'N3-5'\n",
      " 'N4-8' 'N2-7']\n",
      "Validation: Total of 12 cases.\n",
      "['N4-5' 'N3-8' 'N5-5' 'N4-6' 'N1-4' 'N2-9' 'N5-6' 'N4-2' 'N3-3' 'N2-8'\n",
      " 'N1-6' 'N2-3']\n",
      "Test: Total of 11 cases.\n",
      "['N1-3' 'N1-8' 'N1-9' 'N2-5' 'N2-10' 'N3-6' 'N3-7' 'N3-10' 'N4-4' 'N4-7'\n",
      " 'N5-3']\n",
      "\n",
      "Creating image paths...\n",
      "Processed 660 label paths and 660 feature paths.\n",
      "Creating image arrays...\n",
      "Number of label images: 360\n",
      "Completed creating image arrays:\n",
      "Dataset shape  (360, 256, 256, 108)\n",
      "Label image shape  (360, 256, 256)\n",
      "\n",
      "*************************Training*************************\n",
      "x.shape =  torch.Size([2, 108, 256, 256])\n",
      "x.min(), x.max() =  tensor(0.) tensor(1.)\n",
      "y.shape =  torch.Size([2, 256, 256])\n",
      "torch.unique(y) =  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "in constructor inchannel: 108\n",
      "Input channel count3\n",
      "=======start training======\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58f3439aae64dad82fe7858ff456eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae31f786a6be4445992be1bbe09cac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f650427f444238a1ccaa6ec58534bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 1.4893565548790826\n",
      "Validation loss decreased (inf --> 1.489357).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa34064aa8142398ba724ad0346987f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef3a7504df7461d86da934822c663ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 1.1178054514858458\n",
      "Validation loss decreased (1.489357 --> 1.117805).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af391f2f9e6f4bd1bdc49eb5e75c80dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447afaff0fa54b389a2430ec33ecb0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.9440421332915624\n",
      "Validation loss decreased (1.117805 --> 0.944042).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3aab83353543dbbd6d972ffb18a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90bc9ea0fbc466e93cdd71f5d19f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8532590584622489\n",
      "Validation loss decreased (0.944042 --> 0.853259).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3fcad7160a4575bf74fc72479db950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8d5f9d1e2b4465b8f7bf2f95f71cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8475113625327746\n",
      "Validation loss decreased (0.853259 --> 0.847511).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7c039fbc4e44edb5fd894ea5f8cddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a238cd23290e4277a1926b3ded466026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8203831884596083\n",
      "Validation loss decreased (0.847511 --> 0.820383).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54af010fe03d41a4a8199ba6719b2fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6779f315e644d34b8b42a7ecea9855c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8193789217207167\n",
      "Validation loss decreased (0.820383 --> 0.819379).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bca8d9f3594624be6ddf2a9c331612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17f8baecf4d4976a939f7610b605c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8538755560914676\n",
      "EarlyStopping counter: 1 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b9a12ad3c647868ccff3c59f596c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9933803d4b14a67b7d28abf9e209c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8512693377832572\n",
      "EarlyStopping counter: 2 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b27d590ed74bd0b6d91422eefd2e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b7d5ac13b04d1d901691dcf236ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.836930150952604\n",
      "EarlyStopping counter: 3 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2237432b504dfbab38f6466488f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648c6a2f62f49358fc2d0924c1651c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.856280476351579\n",
      "EarlyStopping counter: 4 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb65ce91fa2341c5aefdfe97356387a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fbe49ba71a4025b7163cdef9a0eaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8965649419360691\n",
      "EarlyStopping counter: 5 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38a384ca7ca4988b38d722dc1c3122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfe4bc0eb1e4c02974e1cdf9897248e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.900200521449248\n",
      "EarlyStopping counter: 6 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b90ca4a5d0e42f2a23d006a7598d4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb176492f9df43399c822dc213e4e001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8251511782407761\n",
      "EarlyStopping counter: 7 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fa37affd1349bd87c33dc685ca1c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed00f4b408a24912b8087d99c5e2303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.7310100174612469\n",
      "Validation loss decreased (0.819379 --> 0.731010).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c5b7e1d225468da5c528c82c44211f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c252d52949d459ea71d1f40cc8515f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.6102319363504648\n",
      "Validation loss decreased (0.731010 --> 0.610232).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f36a7aa082640018368f65e1283bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9dd218c2e64ed89d270f3a4d559c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.7413873838881652\n",
      "EarlyStopping counter: 1 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835da2e225a14518a14a71951672cb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9e490c8bc44d08bb301b78b62e454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.8232106913295058\n",
      "EarlyStopping counter: 2 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1abffc139bb4372bff2b577ac5e91da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa6d9efad2e4cd3b0b5acb9548181b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.851857641670439\n",
      "EarlyStopping counter: 3 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09197689f5461c98c9d09708391d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4602b50e16b04a06985339ed21355af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.869391261620654\n",
      "EarlyStopping counter: 4 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be5e25f68a3478aa798dc50966a6b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf8503126bb4e77bd4a01f4980484c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.9282636983527077\n",
      "EarlyStopping counter: 5 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d6e963dedb4742b173f9555eb180df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1828d038f09432f96d7e28d16622843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.9734167497191164\n",
      "EarlyStopping counter: 6 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dabbacdaed4435795acfeb0929c908d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0555b66d7fc408187adeb2651414c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.953361079427931\n",
      "EarlyStopping counter: 7 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be14836c4d604495939b66f24bb9359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6c05f964db404e91164b88dee5f76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.9732205219566822\n",
      "EarlyStopping counter: 8 out of 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15adc8abdacb4293b07a65ea46ff2ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500bf20d62e5401a9b8720ef39c0e8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_losses 0.9980337724089623\n",
      "EarlyStopping counter: 9 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x73c131a42520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9143d33dec6340aea2f4132f2e873940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during training/validation:\n",
      "DataLoader worker (pid(s) 3756309) exited unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/queue.py\", line 179, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/Documents/cervicalResearchIIP/trainer2.py\", line 100, in run_trainer\n",
      "    self._train()\n",
      "  File \"/home/eric/Documents/cervicalResearchIIP/trainer2.py\", line 202, in _train\n",
      "    for i, (x, y) in batch_iter:\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/notebook.py\", line 250, in __iter__\n",
      "    for obj in it:\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1285, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1146, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 3756309) exited unexpectedly\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3756309) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 168\u001b[0m\n\u001b[1;32m    151\u001b[0m input_dataset_val,label_dataset_val \u001b[38;5;241m=\u001b[39m CreateWeightImage(input_name_val, augmentation\u001b[38;5;241m=\u001b[39mAUGMENTED)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# ### lastly added for weight management\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# weights = weights.view(1, 1, 1, -1)  # Reshape for broadcasting\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# input_dataset = input_dataset.numpy()\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# input_dataset_val = input_dataset_val.numpy()\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[43mLearn_EA\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_dataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_dataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m mConv_predict(X_test, fold)\n\u001b[1;32m    171\u001b[0m clear_cuda_memory()\n",
      "Cell \u001b[0;32mIn[18], line 151\u001b[0m, in \u001b[0;36mLearn_EA\u001b[0;34m(input_dataset, label_dataset, input_dataset_val, label_dataset_val, type_number, fold)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======start training======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m training_losses, validation_losses, lr_rates \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m#ここがちゃんとESで最適なエポック数のモデルになっているか要検証\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cervicalResearchIIP/trainer2.py:100\u001b[0m, in \u001b[0;36mTrainer2.run_trainer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# epoch counter\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Training block\"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validation block\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_DataLoader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/cervicalResearchIIP/trainer2.py:202\u001b[0m, in \u001b[0;36mTrainer2._train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# accumulate the losses here\u001b[39;00m\n\u001b[1;32m    198\u001b[0m batch_iter \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_DataLoader), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_DataLoader),\n\u001b[1;32m    199\u001b[0m                   leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 202\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# send to device (GPU or CPU)\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(f'Train Input size: {input.size()}')\u001b[39;49;00m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(f'Train Target size: {target.size()}')\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3756309) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "## Execution\n",
    "from dataArrange import dataRearrange1\n",
    "## Test Learn_EA without annealing\n",
    "# training transformations and augmentations\n",
    "\n",
    "transforms_training = ComposeDouble([\n",
    "    FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "    FunctionWrapperDouble(np.moveaxis, input=True, target=False, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "])\n",
    "\n",
    "#追加箇所ver3\n",
    "transforms_val =  ComposeDouble([\n",
    "    FunctionWrapperDouble(create_dense_target, input=False, target=True),\n",
    "    FunctionWrapperDouble(np.moveaxis, input=True, target=False, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01)\n",
    "\n",
    "])\n",
    "\n",
    "# random seed\n",
    "random_seed = 0\n",
    "\n",
    "X = file_names_with_prefix(data_dir, 'N')\n",
    "\n",
    "#X.remove('N5-2')\n",
    "#X.remove('N5-6')\n",
    "\n",
    "y_file_names = file_names_with_prefix(labeled_dir, 'N')\n",
    "\n",
    "#y_file_names.remove('N5-2')\n",
    "#y_file_names.remove('N5-6')\n",
    "\n",
    "y = [label_group[:2] for label_group in y_file_names]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "##RANDOM WEIGHT TENSOR for now\n",
    "# weights = torch.tensor([\n",
    "#     #'GRY_', 'NML1', 'NML2', 'NML3', 'TOP1', 'TOP2', 'TOP3', 'TOP4', 'SBLX', 'SBLY', \n",
    "#     1.4, 1.4, 1.4, 1.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "#     #'SBLM', 'SBLD', 'SBL1', 'SBL2', 'SBL3', 'SBL4', 'LPL1', 'LPL2', 'MEA1', 'MEA2', \n",
    "#     0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.3, 0.3, 1.4, 1.3,\n",
    "#     #'GAU1', 'GAU2', 'MED1', 'MED2', 'LBP1', 'LBP2', 'LBP3', 'ETC1', 'ETC2', 'STC1', \n",
    "#     1.2, 1.2, 1.4, 1.4, 0.3, 0.3, 0.3, 1.3, 1.3, 1.3,\n",
    "#     #'STC2', 'HGF_', 'NGP_', 'POS1', 'POS2', 'POS3', 'SOL_', 'EMB1', 'EMB2', 'EMB3', \n",
    "#     1.2, 1.3, 2.0, 1.4, 1.4, 1.4, 0.5, 1.3, 1.3, 1.3,\n",
    "#     #'KNN1', 'KNN2', 'BLT1', 'BLT2', 'OOO_', 'CAN1', 'CAN2', 'CAN3', 'PRE1', 'PRE2', \n",
    "#     1.4, 1.4, 1.4, 1.4, -2.0, 0.3, 0.3, 0.3, 1.2, 1.4,\n",
    "#     #'PRE3', 'PRE4', 'UNS1', 'UNS2', 'UNS3', 'UNS4', 'UNS5', 'FOU1', 'FOU2', 'FOU3', \n",
    "#     1.0, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 0.5, 0.5, 0.5,\n",
    "#     #'FOU4', 'ERO1', 'ERO2', 'ERO3', 'ERO4', 'ERO5', 'ERO6', 'OPN1', 'OPN2', 'OPN3', \n",
    "#     0.5, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.4, 1.4, 1.4,\n",
    "#     #'OPN4', 'OPN5', 'CLO1', 'CLO2', 'CLO3', 'CLO4', 'CLO5', 'SCH1', 'SCH2', 'SCH3', \n",
    "#     1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 0.3, 0.3, 0.3,\n",
    "#     #'SCH4', 'ROB1', 'ROB2', 'ROB3', 'ROB4', 'MIN1', 'MIN2', 'MIN3', 'MIN4', 'MAX1', \n",
    "#     0.3, 0.5, 0.5, 0.5, 0.5, 1.4, 1.4, 1.4, 1.4, 1.4,\n",
    "#     #'MAX2', 'MAX3', 'MAX4', 'MRG1', 'MRG2', 'MRG3', 'MRG4', 'MRL1', 'MRL2', 'MRL3', \n",
    "#     1.4, 1.4, 1.4, 0.5, 0.5, 0.5, 0.5, 1.4, 1.4, 1.4,\n",
    "#     #'MRL4', 'BTM1', 'BTM2', 'BTM3', 'BTM4', 'DST_', 'HOM_', 'RIC_'\n",
    "#     1.4, 0.5, 0.5, 0.5, -.5, -2.0, -2.0, 1.4\n",
    "# ])\n",
    "\n",
    "\n",
    "#dices\n",
    "#ablation weight\n",
    "# weights = torch.tensor([\n",
    "#      0.666, 0.603, 0.686, 0.691, 0.62, 0.671, 0.7, 0.661, 0.709, 0.624, \n",
    "#      0.714, 0.728, 0.639, 0.731, 0.704, 0.734, 0.741, 0.731, 0.715, 0.663, \n",
    "#      0.648, 0.692, 0.749, 0.609, 0.744, 0.661, 0.663, 0.723, 0.698, 0.685, 0.722, \n",
    "#      0.641, 0.667, 0.752, 0.654, 0.683, 0.683, 0.18, 0.658, 0.68, 0.677, 0.669, 0.687, \n",
    "#      0.789, 0.717, 0.773, 0.748, 0.675, 0.746, 0.74, 0.694, 0.688, 0.637, 0.63, 0.669, 0.722, \n",
    "#      0.746, 0.705, 0.64, 0.693, 0.662, 0.652, 0.703, 0.744, 0.657, 0.676, 0.676, 0.671, 0.674, \n",
    "#      0.683, 0.679, 0.702, 0.672, 0.681, 0.647, 0.732, 0.735, 0.676, 0.699, 0.651, 0.766, 0.74, \n",
    "#      0.716, 0.634, 0.665, 0.649, 0.761, 0.713, 0.649, 0.72, 0.747, 0.67, 0.667, 0.666, 0.674, \n",
    "#      0.649, 0.637, 0.749, 0.682, 0.721, 0.729, 0.691, 0.74, 0.735, 0.766, 0.761, 0.732, 0.686\n",
    "# ])\n",
    "\n",
    "\n",
    "# #individual filter weights\n",
    "# weights = torch.tensor([\n",
    "#     0.717, 0.704, 0.695, 0.708, 0.467, 0.386, 0.489, 0.633, \n",
    "#     0.479, 0.53, 0.497, 0.464, 0.503, 0.565, 0.45, 0.583, 0.223, \n",
    "#     0.337, 0.708, 0.674, 0.688, 0.651, 0.683, 0.734, 0.39, 0.274, 0.382, \n",
    "#     0.649, 0.669, 0.666, 0.628, 0.67, 0.819, 0.633, 0.641, 0.691, 0.547, \n",
    "#     0.633, 0.645, 0.606, 0.694, 0.684, 0.699, 0.713, 0, 0.398, 0.347, 0.259, \n",
    "#     0.582, 0.733, 0.552, 0.711, 0.689, 0.688, 0.676, 0.71, 0.687, 0.691, 0.383, \n",
    "#     0.375, 0.721, 0.693, 0.727, 0.672, 0.681, 0.67, 0.642, 0.68, 0.702, 0.666, 0.674, \n",
    "#     0.719, 0.706, 0.669, 0.725, 0.734, 0.693, 0.402, 0.308, 0.582, 0.217, 0.467, 0.663, \n",
    "#     0.537, 0.508, 0.691, 0.681, 0.677, 0.659, 0.701, 0.688, 0.652, 0.664, 0.548, 0.588, \n",
    "#     0.614, 0.562, 0.68, 0.709, 0.653, 0.679, 0.357, 0.485, 0.532, 0.581, 0.195, 0.307, 0.699\n",
    "# ])\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# weights = F.softmax(weights, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "if CROSS_VAL:\n",
    "    skf = StratifiedKFold(n_splits=N_SPLIT, shuffle=True)\n",
    "    # each case same division\n",
    "    #skf = StratifiedKFold(n_splits=N_SPLIT, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        if fold < 2:\n",
    "             continue\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Split the training data further into train and validation ( 1/3 split)\n",
    "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.333333333, random_state=42, stratify=y_train)\n",
    "        \n",
    "        \n",
    "        input_train = X_train_final\n",
    "        input_name_val = X_val\n",
    "        print(\"Cross validation: \" + str(CROSS_VAL))\n",
    "        print(f\"Fold: {fold} out of {N_SPLIT}\")\n",
    "        print(\"Augmentation: \" + str(AUGMENTED))\n",
    "        if AUGMENTED:\n",
    "            print(\"Augmentation amount: \" + str(AUGMENTATION))\n",
    "        print(\"Training: Total of \" + str(len(input_train)) + \" cases.\")\n",
    "        print(input_train)\n",
    "\n",
    "        print(\"Validation: Total of \" + str(len(input_name_val)) + \" cases.\")\n",
    "        print(input_name_val)\n",
    "        \n",
    "        print(\"Test: Total of \" + str(len(X_test)) + \" cases.\")  \n",
    "        print(X_test)\n",
    "        print()\n",
    "        \n",
    "        if AUGMENTED:\n",
    "            ## This part is for accounting the data augmentation, attaches augmentation numbers after the original img\n",
    "            ## ex: original number N1-1 -> N1-1-1, N1-1-2 etc.\n",
    "            repeated_items_train = np.repeat(input_train, AUGMENTATION)\n",
    "            suffixes_train = np.tile(np.arange(1, AUGMENTATION + 1), len(repeated_items_train))\n",
    "            input_train = np.array([f\"{item}-{suffix}\" for item, suffix in zip(repeated_items_train, suffixes_train)])\n",
    "            #Custom Arranging for training dataset because of the augmentation order\n",
    "            input_train = np.array(dataRearrange1(input_train, AUGMENTATION))\n",
    "            \n",
    "            \n",
    "            repeated_items_val = np.repeat(input_name_val, AUGMENTATION)\n",
    "            suffixes_val = np.tile(np.arange(1, AUGMENTATION + 1), len(repeated_items_val))\n",
    "            input_name_val = np.array([f\"{item}-{suffix}\" for item, suffix in zip(repeated_items_val, suffixes_val)])\n",
    "        \n",
    "        \n",
    "        input_dataset,label_dataset = CreateWeightImageNew(input_train, augmentation=AUGMENTED, mrcnn=True)\n",
    "        #input_dataset_val,label_dataset_val = CreateWeightImage(input_name_val, augmentation=AUGMENTED)\n",
    "        \n",
    "        # ### lastly added for weight management\n",
    "        # weights = weights.view(1, 1, 1, -1)  # Reshape for broadcasting\n",
    "        \n",
    "        # #making pytorch tensor\n",
    "        # input_dataset = torch.tensor(input_dataset, dtype=torch.float32)\n",
    "        # input_dataset_val = torch.tensor(input_dataset_val, dtype=torch.float32)\n",
    "        # weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        # input_dataset = input_dataset * weights\n",
    "        # input_dataset_val = input_dataset_val * weights\n",
    "        \n",
    "        # input_dataset = input_dataset.numpy()\n",
    "        # input_dataset_val = input_dataset_val.numpy()\n",
    "        \n",
    "        \n",
    "        #Learn_EA(input_dataset,label_dataset,input_dataset_val,label_dataset_val, 0, fold)\n",
    "        #mConv_predict(X_test, fold)\n",
    "        \n",
    "        #clear_cuda_memory()\n",
    "        \n",
    "        input_dataset_val,label_dataset_val = CreateWeightImageNew(input_name_val, augmentation=AUGMENTED, mrcnn=True)\n",
    "        Learn_EA_mask_rcnn(input_dataset,label_dataset,input_dataset_val,label_dataset_val, 0, fold)\n",
    "        clear_cuda_memory()\n",
    "        mConv_predict_mask_rcnn(X_test, fold)\n",
    "        clear_cuda_memory()\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "    \n",
    "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.333333333, random_state=42, stratify=y_train)\n",
    "    \n",
    "    input_train = X_train_final\n",
    "    input_name_val = X_val\n",
    "    \n",
    "    print(\"Cross validation: \" + str(CROSS_VAL))\n",
    "    print(\"Augmentation: \" + str(AUGMENTED))\n",
    "    if AUGMENTED:\n",
    "        print(\"Augmentation amount: \" + str(AUGMENTATION))\n",
    "    print(\"Training: Total of \" + str(len(input_train)) + \" cases.\")\n",
    "    print(input_train)\n",
    "\n",
    "    print(\"Validation: Total of \" + str(len(input_name_val)) + \" cases.\")\n",
    "    print(input_name_val)\n",
    "        \n",
    "    print(\"Test: Total of \" + str(len(X_test)) + \" cases.\")  \n",
    "    print(X_test)\n",
    "    print()\n",
    "    \n",
    "    if AUGMENTED:\n",
    "        ## This part is for accounting the data augmentation, attaches augmentation numbers after the original img\n",
    "        ## ex: original number N1-1 -> N1-1-1, N1-1-2 etc.\n",
    "        repeated_items_train = np.repeat(input_train, AUGMENTATION)\n",
    "        suffixes_train = np.tile(np.arange(1, AUGMENTATION + 1), len(repeated_items_train))\n",
    "        input_train = np.array([f\"{item}-{suffix}\" for item, suffix in zip(repeated_items_train, suffixes_train)])\n",
    "        \n",
    "        #Custom Arranging for training dataset because of the augmentation order\n",
    "        input_train = np.array(dataRearrange1(input_train, AUGMENTATION))\n",
    "            \n",
    "            \n",
    "        repeated_items_val = np.repeat(input_name_val, AUGMENTATION)\n",
    "        suffixes_val = np.tile(np.arange(1, AUGMENTATION + 1), len(repeated_items_val))\n",
    "        input_name_val = np.array([f\"{item}-{suffix}\" for item, suffix in zip(repeated_items_val, suffixes_val)])\n",
    "    \n",
    "    input_dataset,label_dataset = CreateWeightImageNew(input_train, augmentation=AUGMENTED)\n",
    "    input_dataset_val,label_dataset_val = CreateWeightImage(input_name_val, augmentation=AUGMENTED)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # ### lastly added for weight management\n",
    "    # weights = weights.view(1, 1, 1, -1)  # Reshape for broadcasting\n",
    "    \n",
    "    # #making pytorch tensor\n",
    "    # input_dataset = torch.tensor(input_dataset, dtype=torch.float32)\n",
    "    # input_dataset_val = torch.tensor(input_dataset_val, dtype=torch.float32)\n",
    "    # weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    \n",
    "    # input_dataset = input_dataset * weights\n",
    "    # input_dataset_val = input_dataset_val * weights\n",
    "    # input_dataset = input_dataset.numpy()\n",
    "    # input_dataset_val = input_dataset_val.numpy()\n",
    "    Learn_EA(input_dataset,label_dataset,input_dataset_val,label_dataset_val, 0)\n",
    "    mConv_predict(X_test)\n",
    "    clear_cuda_memory()\n",
    "    input_dataset_val,label_dataset_val = CreateWeightImageNew(input_name_val, augmentation=AUGMENTED)\n",
    "    Learn_EA_mask_rcnn(input_dataset,label_dataset,input_dataset_val,label_dataset_val, 0)\n",
    "    clear_cuda_memory()\n",
    "    \n",
    "    mConv_predict_mask_rcnn(X_test)\n",
    "    clear_cuda_memory()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_optuna = pd.DataFrame(columns = ['Dice_importance', 'CE_importance', \"Dice\"])\n",
    "# def objective(trial):\n",
    "#     N_BLOCK = 5\n",
    "#     LR = 0.003\n",
    "\n",
    "#     OUTPUT_DIR = '20250528-DiCELossOptimization'\n",
    "    \n",
    "#     print(f\"Trial {trial.number}\")\n",
    "\n",
    "#     IN_CHANNEL = 108\n",
    "\n",
    "#     AUGMENTED = False\n",
    "#     AUGMENTATION  =  30\n",
    "\n",
    "\n",
    "#     CROSS_VAL = False\n",
    "#     N_SPLIT = 4\n",
    "    \n",
    "#     DICE_IMPORTANCE = trial.suggest_float(\"dice_importance\", 1.0, 100.0)\n",
    "#     CE_IMPORTANCE = trial.suggest_float(\"ce_importance\", 1.0, 100.0)\n",
    "       \n",
    "    \n",
    "#     dice_val = executeExperiment()\n",
    "    \n",
    "#     #df_optuna.add([DICE_IMPORTANCE, CE_IMPORTANCE, dice_val])\n",
    "#     return dice_val\n",
    "    \n",
    "# import optuna\n",
    "\n",
    "# # Create a study to minimize or maximize the metric\n",
    "# study = optuna.create_study(direction='maximize')  # or 'minimize' based on your metric\n",
    "# study.optimize(objective, n_trials = 360)\n",
    "# #df_optuna.to_csv(\"optuna_optimization_loss_function.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(study.best_params)\n",
    "# study.best_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
