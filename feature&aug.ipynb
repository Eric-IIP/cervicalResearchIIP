{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 18:12:31.664076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 18:12:32.743092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from numpy.core.numeric import NaN\n",
    "from MCtool.RFilter import gray\n",
    "from genericpath import exists\n",
    "from matplotlib import image\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras.backend import dtype\n",
    "from DeepLearning import LearnAndTest\n",
    "from Rpkg.Rfund.InputFeature import InputFeature\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Rpkg.Rfund import ReadFile, WriteFile\n",
    "from Rpkg.Rmodel import Unet, Mnet\n",
    "\n",
    "import Filtering\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import DeepLearning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from Rpkg.Rfund.InputFeature import InputFeature\n",
    "from Rpkg.Rfund import ReadFile, WriteFile\n",
    "from Rpkg.Rmodel import Unet, Mnet\n",
    "\n",
    "from MCtool import RFilter, resultEval\n",
    "from DeepLearning import save_eval_result\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from transformations import ComposeDouble, FunctionWrapperDouble, create_dense_target, normalize_01\n",
    "from customdatasets import SegmentationDataSet1\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "from skimage.transform import resize\n",
    "\n",
    "#early stopping なし\n",
    "from unet import UNet\n",
    "from trainer import Trainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is no randomness in the output so that the output is reproduceable\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for Python random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using GPU\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Make the convolution operations deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Disable the CUDNN benchmark to ensure deterministic results\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "['oN4']\n"
     ]
    }
   ],
   "source": [
    "N_BLOCK = 4\n",
    "LR = 0.01\n",
    "OUTPUT_DIR = 666\n",
    "IN_CHANNEL =108\n",
    "\n",
    "AUGMENTATION  =  50\n",
    "\n",
    "\n",
    "def random_test_choose(test_data):\n",
    "    for i in range(len(test_data)):\n",
    "        test_data[i] +=  '-' + str(random.randint(1, AUGMENTATION)) + '.'\n",
    "    return test_data\n",
    "\n",
    "\n",
    "\n",
    "# INPUT = ['N1', 'N2']\n",
    "# VALIDATION = [ 'N3']\n",
    "\n",
    "\n",
    "INPUT = ['N1', 'N2']\n",
    "VALIDATION = [ 'N3']\n",
    "ANNEALING = []\n",
    "\n",
    "# on data aug\n",
    "#TEST = ['oN4']\n",
    "\n",
    "# on original dataset\n",
    "TEST = ['oN4']\n",
    "\n",
    "# Randomly chooses one image from the test augmented image to ensure uniqueness and integrity of the test dataset\n",
    "# For example if the augmentaion scale is 10x, it means there are 10 aug images from single original image\n",
    "# Since we cant include all the image to the testing because the original one image is used 10 times\n",
    "# So random one image is chosen from the aug 10x imgs\n",
    "#TEST = ['N4-1', 'N4-2', 'N4-3', 'N4-4', 'N4-5', 'N4-6', 'N4-7', 'N4-8', 'N4-9']\n",
    "#TEST = random_test_choose(TEST)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Test dataset:')\n",
    "print(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version installed: 2.3.0+cu121\n",
      "CUDA version associated with PyTorch version: 12.1\n",
      "Version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch8902\n",
      "CUDA is available: True\n",
      "Number of GPUs compatible with CUDA:1\n",
      "Name of the GPU at index 0: NVIDIA GeForce RTX 2080 Ti\n",
      "Current CUDA device index: 0\n"
     ]
    }
   ],
   "source": [
    "# 自分の環境設定がうまくいったかどうかを確認しましょう、特にGPUの動作\n",
    "# Prints the version of PyTorch installed\n",
    "print('PyTorch Version installed: ' + torch.__version__)\n",
    "\n",
    "# Prints the version of CUDA associated with the installed PyTorch version\n",
    "print('CUDA version associated with PyTorch version: ' + torch.version.cuda)\n",
    "\n",
    "# Prints the version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch\n",
    "print('Version of cuDNN (CUDA Deep Neural Network library) being used by PyTorch' + str(torch.backends.cudnn.version()))\n",
    "\n",
    "# Same as the line above\n",
    "print('CUDA is available: ' + str(torch.cuda.is_available()))\n",
    "\n",
    "# Returns the number of available CUDA-enabled GPUs\n",
    "print('Number of GPUs compatible with CUDA:' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Returns the name of the GPU at index 0\n",
    "print('Name of the GPU at index 0: '  + str(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# Returns the index of the current CUDA device being used\n",
    "print('Current CUDA device index: '  + str(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル名の先頭部分（prefix）により自動的にファイル名を抽出するアルゴリズム。\n",
    "# 実際それぞれのファイル名は違うと思うので、必須ではない\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Extracts filenames in directory if they start with the prefix input \n",
    "\n",
    "\n",
    "Args/Parameters:\n",
    "\n",
    "    directory_path (string): The path of the dir (ex: /root/home/Documents/etc)\n",
    "    \n",
    "    prefix (string): Prefix of the file name (ex: 'Bo' is a prefix of 'Bone')\n",
    "\n",
    "Returns:\n",
    "\n",
    "    sorted_file_names (list of str): File names sorted in ascending order in the dir without extension ex: ['bone1', 'bone2', ...]\n",
    "\n",
    "Raises:\n",
    "\n",
    "    SomeError: ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def file_names_with_prefix(directory_path, prefix):\n",
    "\n",
    "    # Initialize an empty list to store the file names without extensions\n",
    "    file_names_without_extension = []\n",
    "\n",
    "    # Loop through all files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        #Checking if the file in loop exists in the directory_path not sure how is this necessary\n",
    "        #??\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            # Check if the file name starts with the specified prefix\n",
    "            if filename.startswith(prefix):\n",
    "                # Get the file name without extension\n",
    "                name_without_extension, _ = os.path.splitext(filename)\n",
    "\n",
    "                # Append the file name (without extension) to the list\n",
    "                file_names_without_extension.append(name_without_extension)\n",
    "\n",
    "    # Sort the list of file names without extensions in ascending order\n",
    "    sorted_file_names = sorted(\n",
    "        file_names_without_extension,\n",
    "        key=lambda x: (x.split('-')[0], int(x.split('-')[1]))\n",
    "    )  # Modify this part based on your file naming convention\n",
    "\n",
    "    # Now you have a sorted list of file names with the specified prefix and without extensions\n",
    "    return sorted_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################\n",
    "# ###bugged/ doesnt read by order\n",
    "# ###Custom Function renames all files in format N1-...\n",
    "# ###Only need to run it one time\n",
    "# #############################\n",
    "\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# def rename_images(directory):\n",
    "#     # Get a list of all files in the directory\n",
    "#     files = os.listdir(directory)\n",
    "    \n",
    "#     # Filter out only the image files\n",
    "#     image_files = [file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "    \n",
    "#     # Sort the image files to ensure consistent naming\n",
    "#     image_files.sort()\n",
    "    \n",
    "#     # Initialize counter\n",
    "#     counter = 1\n",
    "    \n",
    "#     # Iterate through the image files\n",
    "#     for filename in image_files:\n",
    "#         # Open the image\n",
    "#         image_path = os.path.join(directory, filename)\n",
    "#         with Image.open(image_path) as img:\n",
    "#             # Rename the image file\n",
    "#             new_filename = f\"N1-{counter}\" + os.path.splitext(filename)[1]\n",
    "#             new_image_path = os.path.join(directory, new_filename)\n",
    "            \n",
    "#             # Save the image with the new name\n",
    "#             img.save(new_image_path)\n",
    "        \n",
    "#         # Increment the counter\n",
    "#         counter += 1\n",
    "\n",
    "# # Specify the directory containing the images\n",
    "# directory_path = \"/home/eric/Downloads/imgNaming/processing\"\n",
    "\n",
    "# # Call the function to rename the images\n",
    "# rename_images(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory: /home/eric/Documents/cervicalResearchIIP\n",
      "Data directory (original dir): /home/eric/Documents/cervicalResearchIIP/img_1006t/dataset_to_add/add_original\n",
      "Feature img directory: /home/eric/Documents/cervicalResearchIIP/img_1006t/dataset_to_add/add_feature\n",
      "Labeled img directory: /home/eric/Documents/cervicalResearchIIP/img_1006t/dataset_to_add/add_label\n",
      "Annealing directory: /home/eric/Documents/cervicalResearchIIP/img_1006t/original\n",
      "Result directory: /home/eric/Documents/cervicalResearchIIP/result/20241128-Conv1x1-666\n",
      "Test result directory: /home/eric/Documents/cervicalResearchIIP/result_test/20241128-Conv1x1-666\n",
      "[]\n",
      "['N3-10']\n",
      "[]\n",
      "[]\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ここで、folder名とかPathとか色々設定\n",
    "\n",
    "# Setting the directory name, path and other settings\n",
    "\n",
    "# Define the root directory where your project is located\n",
    "# Defining a Path object for the project's root dir\n",
    "root_dir = Path(pathlib.Path.cwd())\n",
    "\n",
    "# result folder name\n",
    "date_str = '20241128-Conv1x1-' + str(OUTPUT_DIR)\n",
    "\n",
    "# Define the directories for different types of data\n",
    "# Concatenating the root dir to the different dataset dirs\n",
    "data_dir = str(root_dir / \"img_1006t/dataset_to_add/add_original\")\n",
    "feature_dir = str(root_dir / \"img_1006t/dataset_to_add/add_feature\") \n",
    "labeled_dir = str(root_dir / \"img_1006t/dataset_to_add/add_label\")\n",
    "\n",
    "# test_data_dir = str(root_dir / \"img_1006/originalTest\") \n",
    "# test_labeled_dir = str(root_dir / \"img_1006/testLabel\") \n",
    "# test_feature_dir = str(root_dir / \"img_1006/testFeature\")\n",
    "\n",
    "\n",
    "# annealing_img_dir = str(root_dir / \"img_1006/annealing_img\") # 焼きなまし法時に使う\n",
    "# annealing later, original for now\n",
    "annealing_img_dir = str(root_dir / \"img_1006t/original\")\n",
    "result_dir = str(root_dir / \"result\" / date_str)\n",
    "test_result_dir= str(root_dir / \"result_test\" / date_str)\n",
    "\n",
    "# Making directories based on the path string result_dir and test_result_dir\n",
    "Path(result_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(test_result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prints the paths of the dirs\n",
    "print('Root directory: ' + str(root_dir))\n",
    "print('Data directory (original dir): ' + str(data_dir))\n",
    "print('Feature img directory: ' + str(feature_dir))\n",
    "print('Labeled img directory: ' + str(labeled_dir))\n",
    "print('Annealing directory: ' + str(annealing_img_dir))\n",
    "print('Result directory: ' + str(result_dir))\n",
    "print('Test result directory: ' + str(test_result_dir))\n",
    "\n",
    "# Defining variables filename list of path str starts with the prefix format\n",
    "# In this case: N1 and N3 is training data and N2 is validation data and N4 is a test data\n",
    "input_train = []\n",
    "input_name_val = []\n",
    "annealing_input_name = []\n",
    "input_train = []\n",
    "test_input_name = []\n",
    "for raw_input_img in INPUT:\n",
    "    input_train.extend(file_names_with_prefix(data_dir, raw_input_img))\n",
    "for raw_val_img in VALIDATION:\n",
    "    input_name_val.extend(file_names_with_prefix(data_dir, raw_val_img))\n",
    "for raw_anneal in ANNEALING:\n",
    "    annealing_input_name.extend(file_names_with_prefix(data_dir, raw_anneal))\n",
    "for raw_test in TEST:\n",
    "    test_input_name.extend(file_names_with_prefix(data_dir, raw_test))\n",
    "\n",
    "\n",
    "####old version of assigning\n",
    "# input_train = file_names_with_prefix(data_dir, INPUT)\n",
    "# input_name_val = file_names_with_prefix(data_dir, VALIDATION)\n",
    "# annealing_input_name = file_names_with_prefix(data_dir, ANNEALING)\n",
    "# test_input_name = file_names_with_prefix(data_dir, TEST) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# added this part to investigate the overfitting comment out when it is done\n",
    "#test_input_name = input_train\n",
    "#test_input_name = input_name_val\n",
    "\n",
    "\n",
    "# extra_dataset = file_names_with_prefix(data_dir,'N5-')\n",
    "# input_train.extend(extra_dataset)\n",
    "\n",
    "# Prints the each data image name\n",
    "print(input_train)\n",
    "print(input_name_val)\n",
    "print(annealing_input_name)\n",
    "print(test_input_name)\n",
    "# print(extra_dataset)\n",
    "\n",
    "\n",
    "# Defining a var to store each list length\n",
    "len_train = len(input_train)\n",
    "len_val = len(input_name_val)\n",
    "len_test = len(test_input_name)\n",
    "len_annealing = len(annealing_input_name)\n",
    "\n",
    "\n",
    "print(len(input_train))\n",
    "\n",
    "print(len(input_name_val))\n",
    "print(len(test_input_name))\n",
    "print(len(annealing_input_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GRY_', 'NML1', 'NML2', 'NML3', 'TOP1', 'TOP2', 'TOP3', 'TOP4', 'SBLX', 'SBLY', 'SBLM', 'SBLD', 'SBL1', 'SBL2', 'SBL3', 'SBL4', 'LPL1', 'LPL2', 'MEA1', 'MEA2', 'GAU1', 'GAU2', 'MED1', 'MED2', 'LBP1', 'LBP2', 'LBP3', 'ETC1', 'ETC2', 'STC1', 'STC2', 'HGF_', 'NGP_', 'POS1', 'POS2', 'POS3', 'SOL_', 'EMB1', 'EMB2', 'EMB3', 'KNN1', 'KNN2', 'BLT1', 'BLT2', 'OOO_']\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# 特徴画像の特徴一覧をリストとして取得\n",
    "inputfeature_list = list(map(str, InputFeature))\n",
    "print(inputfeature_list)\n",
    "\n",
    "feature_num = len(inputfeature_list)\n",
    "print(feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 重み計算なし\n",
    "def CreateWeightImage(input_number):\n",
    "    print(\"in create now\")\n",
    "    label_dataset = []\n",
    "    arrDataset = []\n",
    "    for i in input_number:\n",
    "        label_path = os.path.join(labeled_dir, f\"{i}.png\")\n",
    "        input_originallabel = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # commented the binary label because the project has more labels than 2 \n",
    "        #_, binary_label = cv2.threshold(input_originallabel, 0, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        ## This part was used for decreasing and increasing the label count when there was inconsistency with the label dataset\n",
    "        \n",
    "        # if len(np.unique(input_originallabel)) > 11:\n",
    "        #     print(\"Defected image detected (more labels):\" + f\"{i}.png\")\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     for y in range(256):\n",
    "        #         for x in range (256):\n",
    "        #             if (input_originallabel[y][x] == 11) or (input_originallabel[y][x] == 12):\n",
    "        #                 input_originallabel[y][x] = 0\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     cv2.imwrite('testtttt.png', input_originallabel)\n",
    "        # elif len(np.unique(input_originallabel)) < 11:\n",
    "        #     print(\"Defected image detected (Less labels):\" + f\"{i}.png\")\n",
    "        #     defected = f\"{i}_.png\"\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     path_to_mask = '/home/eric/Desktop/edit8label'\n",
    "\n",
    "        #     # 25->9 32->10\n",
    "        #     path_mask_abs = os.path.join(path_to_mask, defected)\n",
    "        #     mask = cv2.imread(path_mask_abs, cv2.IMREAD_GRAYSCALE)\n",
    "        #     for y in range(256):\n",
    "        #         for x in range (256):\n",
    "        #             if (input_originallabel[y][x] == 0) and (mask[y][x] == 25):\n",
    "        #                 input_originallabel[y][x] = 9\n",
    "        #             elif (input_originallabel[y][x] == 0) and (mask[y][x] == 32):\n",
    "        #                 input_originallabel[y][x] = 10\n",
    "        #     print(np.unique(input_originallabel))\n",
    "        #     label_fixed_path = os.path.join(path_to_mask, f\"{i}_fixed.png\")\n",
    "        #     cv2.imwrite(label_fixed_path, input_originallabel)\n",
    "                    \n",
    "        label_dataset.append(input_originallabel)\n",
    "\n",
    "    print(\"Number of label images:\", len(label_dataset))\n",
    "\n",
    "    for i in input_number:\n",
    "        # changed this part from 100 to 256\n",
    "        dataset_img = np.zeros((256, 256, feature_num), dtype=np.float32)\n",
    "\n",
    "        for m in range(feature_num):\n",
    "            feature_img_path = os.path.join(feature_dir, str(i), f\"{inputfeature_list[m]}.png\")\n",
    "            input_featureimg = cv2.imread(feature_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            dataset_img[:, :, m] = input_featureimg\n",
    "\n",
    "        arrDataset.append(dataset_img)\n",
    "\n",
    "    arrDataset = np.array(arrDataset)\n",
    "    print(\"dataset shape \", arrDataset.shape)\n",
    "    print(\"label shape \", np.shape(label_dataset))\n",
    "    print(\"end create now\")\n",
    "\n",
    "    return arrDataset, label_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N3-10', 'N5-1', 'N5-2', 'N5-3', 'N5-4', 'N5-5', 'N5-6']\n",
      "in aug now\n",
      "Number of label images: 7\n",
      "N3-10\n",
      "N5-1\n",
      "N5-2\n",
      "N5-3\n",
      "N5-4\n",
      "N5-5\n",
      "N5-6\n"
     ]
    }
   ],
   "source": [
    "#Augmentation function\n",
    "# need to configure further to make it receive the new shape of the label images\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "data_dir = str(root_dir / \"img_1006t/dataset_to_add/add_original\")\n",
    "feature_dir = str(root_dir / \"img_1006t/dataset_to_add/add_feature\") \n",
    "labeled_dir = str(root_dir / \"img_1006t/dataset_to_add/add_label\")\n",
    "\n",
    "\n",
    "test_data_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Original\") \n",
    "test_labeled_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Label\") \n",
    "test_feature_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Feature\")\n",
    "\n",
    "## 重み計算なし\n",
    "def ImageAug(input_number):\n",
    "    print(\"in aug now\")\n",
    "    label_dataset = []\n",
    "    arrDataset = []\n",
    "    for i in input_number:\n",
    "        label_path = os.path.join(labeled_dir, f\"{i}.png\")\n",
    "        input_originallabel = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        label_dataset.append(input_originallabel)\n",
    "\n",
    "\n",
    "    print(\"Number of label images:\", len(label_dataset))\n",
    "    \n",
    "    #means x augment\n",
    "    augmentation_scale = 10\n",
    "    \n",
    "    seq = iaa.Sequential([\n",
    "    #iaa.Fliplr(0.5),  # horizontal flip with 50% probability\n",
    "    #iaa.Flipud(0.5),  # vertical flip with 50% probability\n",
    "    iaa.Affine(\n",
    "        rotate=(-10, 10),  # rotate by -10 to +10 degrees\n",
    "        scale=(0.9, 1.1),  # scale by 90% to 110%\n",
    "        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)}  # translate by -5% to +5%\n",
    "    ),\n",
    "    #iaa.ElasticTransformation(alpha = 3, sigma = 5),  # elastic deformation\n",
    "    #iaa.Multiply((0.8, 1.2)),  # change brightness\n",
    "    #iaa.LinearContrast((0.75, 1.5))  # change contrast\n",
    "])\n",
    "\n",
    "    \n",
    "    arr_labelDataset = []\n",
    "    for i in input_number:\n",
    "        print(i)\n",
    "        original_img_path = os.path.join(data_dir, str(i) + \".png\")\n",
    "        input_original_img = cv2.imread(original_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        for k in range(augmentation_scale):\n",
    "            augmented = seq(image = input_original_img, segmentation_maps = SegmentationMapsOnImage(label_dataset[input_number.index(i)], shape = label_dataset[input_number.index(i)].shape))\n",
    "            arrDataset.append(np.array(augmented[0]))\n",
    "            arr_labelDataset.append(np.array(augmented[1].get_arr()))\n",
    "            \n",
    "        #arrDataset = np.array(arrDataset)\n",
    "        #arr_labelDataset = np.array(arr_labelDataset)\n",
    "        for c, aug_data, aug_label in zip(range(1,augmentation_scale + 1), arrDataset, arr_labelDataset):\n",
    "            path1 = os.path.join(test_data_dir, (i + '-' + str(c)+'.png'))\n",
    "            path2 = os.path.join(test_labeled_dir, (i + '-' + str(c)+'.png'))\n",
    "            #print(path1)\n",
    "            cv2.imwrite(path1, aug_data)\n",
    "            cv2.imwrite(path2, aug_label)\n",
    "        \n",
    "        arrDataset.clear()\n",
    "        arr_labelDataset.clear()\n",
    "print(file_names_with_prefix(data_dir, ''))\n",
    "ImageAug(file_names_with_prefix(data_dir, ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "1: N5-1-3\n",
      "2: N5-1-7\n",
      "3: N5-2-5\n",
      "4: N5-6-4\n",
      "5: N5-6-3\n",
      "6: N5-2-4\n",
      "7: N5-4-5\n",
      "8: N5-3-2\n",
      "9: N5-2-10\n",
      "10: N5-1-8\n",
      "11: N5-3-10\n",
      "12: N5-2-2\n",
      "13: N5-1-4\n",
      "14: N5-6-8\n",
      "15: N5-1-10\n",
      "16: N3-10-5\n",
      "17: N3-10-4\n",
      "18: N3-10-6\n",
      "19: N5-4-8\n",
      "20: N5-5-8\n",
      "21: N3-10-7\n",
      "22: N5-1-5\n",
      "23: N5-4-3\n",
      "24: N5-5-5\n",
      "25: N5-2-3\n",
      "26: N5-6-6\n",
      "27: N5-4-6\n",
      "28: N3-10-8\n",
      "29: N5-4-10\n",
      "30: N5-2-7\n",
      "31: N5-5-6\n",
      "32: N5-2-6\n",
      "33: N5-4-2\n",
      "34: N3-10-2\n",
      "35: N5-1-6\n",
      "36: N5-1-9\n",
      "37: N5-6-7\n",
      "38: N5-4-7\n",
      "39: N5-4-9\n",
      "40: N5-1-2\n",
      "41: N5-6-5\n",
      "42: N5-3-4\n",
      "43: N3-10-10\n",
      "44: N5-3-3\n",
      "45: N5-3-8\n",
      "46: N5-3-6\n",
      "47: N5-3-7\n",
      "48: N5-1-1\n",
      "49: N5-2-8\n",
      "50: N5-6-2\n",
      "51: N5-6-1\n",
      "52: N5-4-1\n",
      "53: N5-2-9\n",
      "54: N5-4-4\n",
      "55: N3-10-3\n",
      "56: N5-2-1\n",
      "57: N5-5-3\n",
      "58: N3-10-9\n",
      "59: N5-3-9\n",
      "60: N5-5-1\n",
      "61: N5-3-1\n",
      "62: N5-5-10\n",
      "63: N3-10-1\n",
      "64: N5-3-5\n",
      "65: N5-5-9\n",
      "66: N5-6-9\n",
      "67: N5-5-2\n",
      "68: N5-5-7\n",
      "69: N5-6-10\n",
      "70: N5-5-4\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "##Custom Function generates feature img\n",
    "#############################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Original\")\n",
    "feature_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Feature\") \n",
    "labeled_dir = str(root_dir / \"img_1006t/dataset_to_add/aug_Label\")\n",
    "\n",
    "def read_images_from_directory(dir_path):\n",
    "    IMAGE_SIZE = 256\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Reads all image files from a directory and returns a list of images as NumPy arrays.\n",
    "    \n",
    "    :param dir_path: Path to the directory containing the images.\n",
    "    :param image_size: Size of the images (assumes square images).\n",
    "    \n",
    "    :return: img_list: List of images as NumPy arrays.\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "    fn_without_list = []\n",
    "\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(dir_path):\n",
    "        # Check if the file is an image file (you can add more extensions if needed)\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Construct the full path to the image\n",
    "            img_path = os.path.join(dir_path, filename)\n",
    "            \n",
    "            # Open the image and resize it to the desired size\n",
    "            img = Image.open(img_path).convert('L')\n",
    "            img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            \n",
    "            # Convert the image to a NumPy array \n",
    "            img_array = np.asarray(img, dtype=np.float64)\n",
    "            \n",
    "            # Append the image array to the list\n",
    "            img_list.append(img_array)\n",
    "            # Gets the file name without extension for creating dir \n",
    "            filename_without_extension = os.path.splitext(filename)[0]\n",
    "            fn_without_list.append(filename_without_extension)\n",
    "    \n",
    "    return img_list, fn_without_list\n",
    "\n",
    "img_list, fn_without_list = read_images_from_directory(data_dir)\n",
    "print(len(fn_without_list))\n",
    "#print(fn_without_list[0])\n",
    "\n",
    "counter = 0\n",
    "for fn_wthout, img in zip(fn_without_list, img_list):\n",
    "    #print(\"File that is being processed at the moment: \" + str(fn_wthout))\n",
    "    if os.path.exists(os.path.join(feature_dir, fn_wthout)):\n",
    "        files = os.listdir(os.path.join(feature_dir, fn_wthout))\n",
    "        png_files = [file for file in files if file.lower().endswith('.png')]\n",
    "        if png_files:\n",
    "            print(fn_wthout + ' already generated dataset, skipping...')\n",
    "            continue\n",
    "    WriteFile.make_folder(feature_dir, '/' + fn_wthout)\n",
    "    fn_feature_list = []\n",
    "    filtered_img_list = []\n",
    "    for feature in InputFeature:\n",
    "        #print('Processed feature now:' + str(feature))\n",
    "        filtered_img= Filtering.single_image(img, feature)\n",
    "        filtered_img_list.append(filtered_img)\n",
    "        fn_feature_list.append(str(feature) + '.png')\n",
    "    WriteFile.save_images(os.path.join(feature_dir,fn_wthout), fn_feature_list, filtered_img_list)\n",
    "    counter += 1 \n",
    "    print(str(counter) + \": \" + fn_wthout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eric/Documents/cervicalResearchIIP/img_1006t/testFeature/N4-9-1\n",
      "['TOP3.png', 'MAX2.png', 'MRL2.png', 'UNS2.png', 'MIN2.png', 'SBL1.png', 'MRL3.png', 'FOU4.png', 'SBL2.png', 'NML2.png', 'ERO2.png', 'RIC_.png', 'BLT1.png', 'SBLM.png', 'LPL2.png', 'PRE4.png', 'GRY_.png', 'NGP_.png', 'FOU1.png', 'FOU3.png', 'NML3.png', 'POS1.png', 'LBP3.png', 'ERO5.png', 'OOO_.png', 'TOP2.png', 'MED1.png', 'OPN2.png', 'SBLY.png', 'MIN3.png', 'BTM3.png', 'ROB1.png', 'PRE2.png', 'MAX3.png', 'BLT2.png', 'FOU2.png', 'OPN3.png', 'BTM4.png', 'ERO3.png', 'HOM_.png', 'ETC1.png', 'MEA1.png', 'ROB3.png', 'SCH2.png', 'CAN2.png', 'UNS3.png', 'MAX1.png', 'CLO3.png', 'KNN1.png', 'SCH3.png', 'CAN3.png', 'POS3.png', 'CLO4.png', 'CLO1.png', 'UNS5.png', 'SCH1.png', 'ERO6.png', 'CLO5.png', 'STC1.png', 'LPL1.png', 'EMB1.png', 'HGF_.png', 'MIN4.png', 'SCH4.png', 'MRG3.png', 'LBP1.png', 'MAX4.png', 'SOL_.png', 'ROB4.png', 'EMB3.png', 'TOP4.png', 'STC2.png', 'BTM1.png', 'UNS4.png', 'TOP1.png', 'NML1.png', 'SBLX.png', 'ROB2.png', 'POS2.png', 'SBL4.png', 'BTM2.png', 'GAU1.png', 'MED2.png', 'MRG1.png', 'OPN5.png', 'CAN1.png', 'UNS1.png', 'CLO2.png', 'SBLD.png', 'SBL3.png', 'ERO4.png', 'LBP2.png', 'EMB2.png', 'GAU2.png', 'MRL4.png', 'ERO1.png', 'MEA2.png', 'KNN2.png', 'PRE3.png', 'OPN1.png', 'MRG4.png', 'OPN4.png', 'PRE1.png', 'MRG2.png', 'ETC2.png', 'MRL1.png', 'DST_.png', 'MIN1.png']\n",
      "['TOP3.png', 'MAX2.png', 'MRL2.png', 'UNS2.png', 'MIN2.png', 'SBL1.png', 'MRL3.png', 'FOU4.png', 'SBL2.png', 'NML2.png', 'ERO2.png', 'RIC_.png', 'BLT1.png', 'SBLM.png', 'LPL2.png', 'PRE4.png', 'GRY_.png', 'NGP_.png', 'FOU1.png', 'FOU3.png', 'NML3.png', 'POS1.png', 'LBP3.png', 'ERO5.png', 'OOO_.png', 'TOP2.png', 'MED1.png', 'OPN2.png', 'SBLY.png', 'MIN3.png', 'BTM3.png', 'ROB1.png', 'PRE2.png', 'MAX3.png', 'BLT2.png', 'FOU2.png', 'OPN3.png', 'BTM4.png', 'ERO3.png', 'HOM_.png', 'ETC1.png', 'MEA1.png', 'ROB3.png', 'SCH2.png', 'CAN2.png', 'UNS3.png', 'MAX1.png', 'CLO3.png', 'KNN1.png', 'SCH3.png', 'CAN3.png', 'POS3.png', 'CLO4.png', 'CLO1.png', 'UNS5.png', 'SCH1.png', 'ERO6.png', 'CLO5.png', 'STC1.png', 'LPL1.png', 'EMB1.png', 'HGF_.png', 'MIN4.png', 'SCH4.png', 'MRG3.png', 'LBP1.png', 'MAX4.png', 'SOL_.png', 'ROB4.png', 'EMB3.png', 'TOP4.png', 'STC2.png', 'BTM1.png', 'UNS4.png', 'TOP1.png', 'NML1.png', 'SBLX.png', 'ROB2.png', 'POS2.png', 'SBL4.png', 'BTM2.png', 'GAU1.png', 'MED2.png', 'MRG1.png', 'OPN5.png', 'CAN1.png', 'UNS1.png', 'CLO2.png', 'SBLD.png', 'SBL3.png', 'ERO4.png', 'LBP2.png', 'EMB2.png', 'GAU2.png', 'MRL4.png', 'ERO1.png', 'MEA2.png', 'KNN2.png', 'PRE3.png', 'OPN1.png', 'MRG4.png', 'OPN4.png', 'PRE1.png', 'MRG2.png', 'ETC2.png', 'MRL1.png', 'DST_.png', 'MIN1.png']\n"
     ]
    }
   ],
   "source": [
    "# fn_wthout = 'N4-9-1'\n",
    "# print(os.path.join(feature_dir, fn_wthout))\n",
    "# path = os.path.join(feature_dir, fn_wthout)\n",
    "# if(os.path.exists(path)):\n",
    "#     files = os.listdir(os.path.join(feature_dir, fn_wthout))\n",
    "#     print(files)\n",
    "#     png_files = [file for file in files if file.lower().endswith('.png')]\n",
    "#     print(png_files)\n",
    "#     png_files.clear()\n",
    "#     if(png_files):\n",
    "#         print('yes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
